Government Information Quarterly
Volume 37, Issue 2, April 2020, 101457
Risk-based data analytics in the government
sector: A case study for a U.S. county
Andrea M. Rozario , Hussein Issa
Show more
https://doi.org/10.1016/j.giq.2020.101457
Get rights and content
Highlights
• Proposed a risk-based exception prioritization conceptual
framework to improve government audits’ quality
• Proposed framework aims to address the problem of information
overload, grounded in processing fluency theory
• Framework applied to the duplicate payments in a procurement
dataset of a U.S. county
• Results demonstrate substantial improvements compared to both
record scanning and traditional sample-based approaches
• The proposed approach has the potential to improve the efficiency
and effectiveness of government audits
Abstract
b a
Share Cite
This study explores the use of
data analytics
for improving the quality of governmentaudits through the lens of processing fluency theory as the driver behind the need for
data analytics
. Little is known about the benefits of
data analytics
to
governmentexpenditure
audits in a data-rich environment. Accordingly, this study proposes a risk-based prioritization framework and applies it to the real procurement dataset of a UScounty. The results indicate that the framework increases the efficiency and effectivenessof identifying true duplicate payments compared to either the scanning or samplingbenchmarks. Specifically, it significantly reduces the number of potential duplicatecandidates that require auditor review to approximately 12% of the
duplicate records
. Assuch, it enables the capture of true duplicates in a shorter period. These results suggestthat the framework offers one way to mitigate the low processing fluency effect ofinformation overload on auditor judgment.
Introduction
Government programs are financed by payments from taxpayers, and as a result, it isessential to assure that their funds are expended appropriately (Appelbaum et al., 2020,Mead, 2008). In fact, government auditors serve as one of the primary governancemechanisms to assure that taxpayers' dollars are not subject to fraud waste and abuse(Deloitte, 2011; Goodwin, 2004; Mead, 2001; Mead, 2002). There is even more scrutinyon holding governments accountable since the 2008 financial crisis (Kozlowski, Issa, &Appelbaum, 2018) from which many government entities continue to recover (WSJ,2015). Accordingly, it is important to understand how government audits can beimproved.
The advent of digitization of business processes and data analytics may be one avenue toachieve more efficient and effective monitoring of government expenditures(Appelbaum, Kogan, & Vasarhelyi, 2017; Gil-Garcia, Helbig, & Ojo, 2014). However, theadvent of digitization has also led to the generation, capture, and storage of voluminousdata. Large amounts of audit relevant information, including payment records fromrelational databases that are stored in near-real-time, make it challenging for auditors toprocess it. Specifically, while data analytics may improve government audits byidentifying potentially anomalous expenditures in the population of payment records,auditors may encounter difficulties in the ease of processing such information, i.e., lowprocessing fluency, due to the limited cognitive processing capabilities of humans(Brown-Liburd, Issa, & Lombardi, 2015; Rennekamp, 2012). Low processing fluency, as aresult, may lead to suboptimal decisions (Murayama, Blake, Kerr, & Castel, 2016; Andiola,Brink, Lynch, & Ferguson, 2019).
To provide insights into the benefits of data analytics to government audits in a data-richenvironment, it is important to consider ways that could increase the ease of information
processing and improve audit quality. Our study explores the following researchquestion: how can the use of prioritization-based data analytics empower the auditor toovercome the challenge of information overload, consequently leading to higher qualityaudits of government expenditures? To answer this question, we adopt a case studyapproach that consists of a conceptual risk-based prioritization framework that aims todetect inappropriate government expenditures, hereafter referred to as ‘exceptions’ andsubsequently illustrate its application using the procurement dataset of a U.S. county.More specifically, we use the setting of duplicate payments, as exceptions, to evaluate theframework's ability to improve audit efficiency and effectiveness.
The implementation of data analytics and the concept of continuous auditing (CA)systems, has the potential to improve audit efficiency
and assist in the timelierdetection of exceptions (e.g., Alles, Brennan, Kogan, & Vasarhelyi, 2006; Alles, Kogan, &Vasarhelyi, 2008; Vasarhelyi & Halper, 1991), which may lead to more effective audits.
While useful, the main drawback of CA systems and other technology-based dataanalytic audit tools is that they can produce an overwhelming amount of exceptions,making it challenging for auditors to analyze (Alles et al., 2006; Alles et al., 2008;Debreceny, Gray, Tham, Goh, & Tang, 2003; Perols & Murthy, 2012).
A couple of studies have proposed conceptual frameworks that can help auditorsprioritize exceptions into varying levels of risk, such as riskier and less risky exceptions(Issa, 2013; Li et al., 2016). Issa (2013) proposes a conceptual framework that leveragesrisk-based criteria and expert elicitation to prioritize duplicate payments. Similarly, Li,Chan, & Kogan, 2016 propose a framework that leverages risk-based criteria and belieffunctions to prioritize them. However, the usability of these frameworks has not beenapplied to real datasets, and they are not clearly grounded in theory, i.e., the need todevelop the research artifact to address the challenge of cognitive processing in avoluminous data environment. Hence, it is vital to explore the use of prioritization-baseddata analytics to enhance the ease to which auditors can process large volumes ofpayments.
Government audits present the optimal opportunity to explore the usability of aprioritization framework that could mitigate the adverse effect of information overloadon auditor judgment. As a result, this case study proposes and applies an innovative dataanalytic technique to the procurement dataset of a U.S. county. Doing so could lead to notonly more efficient but also more effective and higher quality audits since the use of theproposed analytic technique should improve judgments about inappropriate governmentexpenditures.
Although our study is closely related to the Issa (2013) and Li et al., 2016 studies in that itproposes a risk-based prioritization framework to improve audit efficiency and
1
2
effectiveness, it more closely reflects the Issa (2013) framework while establishingseveral additional perspectives. Issa (2013) employs a weighting system derived from anexpert panel, whereas Li et al., 2016 utilize belief functions for the prioritization ofaccounting records, which is not directly observable.
As a result, the main drawback ofthe Li et al., 2016 framework is that it lacks interpretability (transparency) with respectto the derivation of the weights assigned to risk-based criteria. To facilitate thedocumentation of audit evidence, which is paramount in the conduct of audits, our studymodifies and expands the Issa (2013) framework as it is the more interpretable approach.In particular, we expand on Issa (2013) by 1) modifying some of the existing criteria, 2)incorporating additional criteria, and 3) by applying it to a real dataset.
This study examines the application of data analytics in the government sector. As such,it contributes to the government accounting literature by proposing a risk-basedframework to improve the efficiency and effectiveness of audits of governmentexpenditures. Moreover, the application of the proposed prioritization approach has thepotential to mitigate the problem of information overload and low processing fluencythat exposure to large datasets can create. Finally, this study suggests several promisingresearch avenues that future studies can explore.
The remainder of this study is organized as follows. The next section presents a review ofthe literature related to internal audit and data analytics in the government sector. Thethird section describes the methodology. The fourth section presents the results of theapplication of the framework. The fifth section discusses the results. Finally, the lastsection concludes the paper and discusses its limitations as well as avenues for futureresearch.
Section snippets
Literature review
Since the 2008 financial crisis, constituents have demanded better accountability inregards to government reporting (Kozlowski et al., 2018). Therefore, it is vital for auditorsin the public sector to provide high-quality audits. The integration of data analytics invarious audit tasks has the potential to improve government audit efficiency and
3
4
Access through your organization
Check access to the full text by signing in through your organization.
Access through
your organization
effectiveness by detecting improper expenditure of government funds in a timeliermanner while analyzing the entire population of records. However, …
Duplicate payment prioritization framework
Fig. 1 illustrates the conceptual prioritization framework, which consists of three phases.In Phase 1, a rule-based system (comprising of a set of data analytics) performs theduplicate detection test by performing a 2-way match. In Phase 2, the identifiedduplicate instances are prioritized using risk-based criteria. Finally, in Phase 3, theprioritized instances are reviewed by the internal auditor to determine whether themodel captures true duplicate payments. The latter are the duplicate …
Results
This section reports the main findings of our analysis. The discussion and interpretationof these results will follow in Section 5. …
Discussion
The risk-based prioritization framework can be used as an innovative approach toimprove the efficiency and effectiveness of government audits. Consequently, this canhelp auditors better understand how taxpayer's funds are expended and mitigate the riskthat taxpayer's funds are subject to fraud, abuse, and waste.
With respect to audit efficiency and effectiveness, it is clear that the framework canassist in reducing the number of records that would have to be investigated and yet beable to …
Summary
This study is motivated by the digitization of business processes in the Now-Economy,which has led to a data-rich environment, and the increased demand for higher qualityaudits by constituents. Extant research calls for further examination of the impact of dataanalytics on audits in the public sector. In this paper, a risk-based prioritizationframework is proposed to demonstrate the improvement that data analytics can bring toimprove the quality of government audits including improvements …
Declaration of Competing Interest
None. …
Andrea M. Rozario
is a PhD of Accounting Information Systems and is currently atStevens Institute of Technology. She is a licensed CPA in the state of New Jersey and hasworked as an Experienced Assurance Associate in PwC and as a Senior FinancialAnalyst in Quest Diagnostics. Her research focuses on using disruptive technologiesand social media information to enhance audit quality. She has written and presentedpapers in the areas of audit data analytics, blockchain, smart contracts, robotic …
…
…
Recommended articles
References
(56)
M.
Alles
et al.
Continuous monitoring of business process controls: a pilot implementation ofa continuous auditing system at Siemens
International Journal of Accounting Information Systems
(2006)
A.T.
Chatfi eld
et al.
Customer agility and responsiveness through big data analytics for public valuecreation: a case study of Houston 311 on-demand services
Government Information Quarterly
(2018)
S.J.
Eom
et al.
Can social media increase government responsiveness? a case study of Seoul,Korea
Government Information Quarterly
(2018)
J.R.
Gil-Garcia
et al.
Being smart: Emerging technologies and innovation in the public sector
Government Information Quarterly
(2014)
L.
Hagen
et al.
Open data visualizations and analytics as tools for policy-making
Government Information Quarterly
(2019)
E.R.
Iselin
The effects of information load and information diversity on decision quality ina structured decision task
Accounting, Organizations and Society
(1988)
M.
Kassen
A promising phenomenon of open data: a case study of the Chicago open dataproject
Government Information Quarterly
(2013)
K.J.
Knapp
et al.
Key issues in data center security: an investigation of government audit reports
Government Information Quarterly
(2011)
T.
Nam
et al.
The changing face of a city government: a case study of Philly311
Government Information Quarterly
(2014)
J.
Pearl
Reasoning with belief functions: an analysis of compatibility
International Journal of Approximate Reasoning
(1990)
View more references
Cited by (7)
Technology-based boards and bank operational efficiency: Mediating effects ofrisk-taking
2025, Finance Research Letters
Show abstract
The application of continuous audit and monitoring methodology: Agovernment medication procurement case
2024, International Journal of Accounting Information Systems
Citation Excerpt :
…
Therefore, limited audit budgets and resources bring government audits under challenge, andthere is a strong demand to adopt novel approaches to address these challenges in governmentaudits. Although audit data analytics may be one path to achieve more efficient and effectivemonitoring of government spending, this is still a largely neglected area for research andapplications, compared to the private sector (Appelbaum et al., 2017; Rozario & Issa, 2020).Continuous audit and monitoring, as one example of audit data analytics, can assist in the timelydetection of exceptions and anomalies (Vasarhelyi & Halper, 1991).…
Show abstract
Fraud analytics practices in public-sector transactions: a systematic review
2023, Journal of Public Budgeting, Accounting and Financial Management
DOES AUDITOR’S ATTRIBUTES IMPACT ON PROFESSIONAL JUDGEMENT IN AFINANCIAL AUDIT? EMPIRICAL EVIDENCE FROM MYANMAR SAI
2022, Business: Theory and Practice
Issues and Challenges of the Data Analytics Development Project in the Centerof Information System and Financial Technology
2022, 2022 1st International Conference on Information System and Information Technology,ICISIT 2022
Using Artificial Intelligence Technology for Decision Support System in AuditRisk Assessment: A Review Paper
2021, Proceedings - 2021 IEEE 5th International Conference on Information Technology,Information Systems and Electrical Engineering: Applying Data Science and Artifi cial IntelligenceTechnologies for Global Challenges During Pandemic Era, ICITISEE 2021
View all citing articles on Scopus
Andrea M. Rozario
is a PhD of Accounting Information Systems and iscurrently at Stevens Institute of Technology. She is a licensed CPA inthe state of New Jersey and has worked as an Experienced AssuranceAssociate in PwC and as a Senior Financial Analyst in QuestDiagnostics. Her research focuses on using disruptive technologiesand social media information to enhance audit quality. She haswritten and presented papers in the areas of audit data analytics,blockchain, smart contracts, robotic process automation (RPA), andbig data in auditing. Andrea has designed and instructed lectures onaudit data analytics for the Public Company Accounting OversightBoard (PCAOB) and designed webcasts on RPA, blockchain, and smartcontracts for the American Institute of Certifi ed Public Accountants(AICPA). She teaches Auditing and Introduction to FinancialAccounting.
Hussein Issa
received his PhD in Accounting Information Systems fromRutgers University in October 2013. He led numerous research projectswith banks, large multinational consumer products companies, atelecommunication company, one of the Big Ten university’s internalaudit department, among others. His research focused on theapplications of artifi cial intelligence and machine learning to identifyand prioritize exceptions (which was the topic of his dissertation"Exceptional Exceptions"). In these projects, Hussein developed variousstatistical and machine learning models to address business problems,such as fraud detection, outlier identifi cation, pattern recognition,operational effi ciency, customer profi ling, continuous controlmonitoring of procurement systems, among others. Hussein has alsorecently taken interest in text mining and sentiment analysis. Husseinalso serves on the dissertation committees of several PhD students.Hussein teaches/taught Advanced Design and Development ofInformation System (Masters in Accountancy in GovernmentalAccounting), Design and Development of Information System (Mastersin Accountancy in Governmental Accounting), Information Technologyin the Digital Era (Professional Accounting MBA), Introduction toFinancial Accounting (Undergraduate). He also co-teaches a PhD levelcourse discussing current topics in AIS Research. Hussein is fl uent inArabic, English, and French, and intermediate in Russian and Wolof(Senegalese dialect).
View full text
© 2020 Elsevier Inc. All rights reserved.
All content on this site: Copyright ©
2025
Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AItraining, and similar technologies. For all open access content, the relevant licensing terms apply.

================================================================================================================================================================================
A Guide to Government Analytics
Home 
(Excerpts from Chapter 2 of the Handbook)
Government analytics refers to the use of data to diagnose and
improve the machinery of government, or public administration.
The Handbook introduces a public administration production function to provide
an overarching framework to organize the various data sources assessed in
different chapters. A production function relates input factors of production to
the output of deliverables of an organization, and their eventual outcomes. The
productivity of an organization thus depends on the quality and quantity of
outputs relative to inputs. Figure 2.1 visualizes the different components of our
production function for public administration (Meyer-Sahling et al. 2021; World
Bank Group 2019).
FIGURE 2.1 The Public Administration Production Function
While many core elements coincide with typical private sector production
functions (Mas-Colell, Whinston, and Green 1995), the functioning of government
administration has been characterized as distinct from that of private firms due
to the multiplicity of principals, the ambiguity of tasks, and the presence of
The Government Analytics Handbook
This site uses cookies to optimize functionality and give you the best possible experience.
If you continue to navigate this website beyond this page, cookies will be placed on your
browser. To learn more about cookies, click here.
×
principals with political incentives, among other features. In publicadministration, inputs include personnel (public employees), goods (such ascomputers), and capital (such as office space).
Outputs refer to, first, the deliverables produced by public administrationorganizations themselves. For instance, a ministry of finance might issue publicsector debt at a certain interest rate. Further, public administration organizationsproduce outputs (activities) that enable frontline agencies in the public sector—such as hospitals, schools, or police forces—to deliver services and goods tocitizens. The outcomes in these examples are better health, education, or publicsafety, respectively. To fund the outputs, a ministry of finance may overseebudgets that frontline agencies then disburse to deliver their services.
How do public administrations convert inputs (such aspersonnel) into outputs and outcomes?
In our production function, this conversion is enabled by policies (organizationalobjectives and work procedures), systems, and management practices, andmediated by norms and behaviors inside public administration. For instance, aministry of finance may have a policy in place to review a budget for anorganization by a certain date. A team lead inside the ministry then managesemployees to ensure the task is completed well and on time—such as througheffective performance management practices. Those practices and organizationalpolicies shape the norms and behaviors of the ministry’s employees—such astheir motivation to work hard—which in turn then allows the ministry to produceoutputs (such as a budget review).
By utilizing different data sources and different methods,government analytics can shed light on all parts of thisproduction function and identify bottlenecks, whetheroverpriced input goods, ghost workers on the payroll, high staffturnover, or slow processing of administrative cases, to namejust a few.
Contemplating government analytics along the production function enablesanalysts to diagnose public administration challenges holistically, and tounderstand how different data and approaches to government analytics relate.
This site uses cookies to optimize functionality and give you the best possible experience.If you continue to navigate this website beyond this page, cookies will be placed on yourbrowser. To learn more about cookies,
click here
.
To illustrate, figure 2.2 maps a number of different data sources analyzed invarious chapters to their respective components in the production function.Several types of administrative data have particular strengths in diagnosinginputs into the public administration production function. For instance, payrolldata and human resources management information system (HRMIS) data canhelp governments understand personnel as an input into the publicadministration production function, such as whether pay of public servants iscompetitive and fiscally sustainable, or whether staffing levels are adequate (see
chapters 9
and
10
). Budget data and procurement data can help governmentsunderstand spending on goods and capital as inputs into public administration—for instance, whether public administrations acquire similar goods cost-effectively across organizations in the public administration (see chapters 11 and12).
Government analytics can also shed light on the processes and practices thatconvert inputs into outputs and outcomes. Surveys of public servants andqualitative measurement have particular strengths at diagnosing managementpractices. Management quality is fundamentally experienced by employees and aresult from the interaction between managers and employees. Surveys can, forinstance, ask public servants how they perceive the leadership of their superioror the quality of their performance feedback (see
chapter 18
). Governmentanalytics can also shed light on the quality of processes inside publicadministration, such as whether these processes adhere to governmentprocedure or meet deadlines (see
chapter 13
).
Whether practices and processes effectively turn inputs into outputs andoutcomes is, as noted, mediated by the norms, attitudes, and behaviors of publicadministrators. Surveys of public servants and qualitative measurement arestandard practice in many governments to evaluate this component of publicadministration production—for instance, to understand how engaged,committed, and ethical in their behavior public administrators are (see, forexample,
chapter 18
). HRMIS data often complement rich survey data byproviding insights into specific behaviors of public employees that are digitallyrecorded, such as whether public servants leave the organization, work overtime,or take sick leave (see
chapter 9
).
This site uses cookies to optimize functionality and give you the best possible experience.If you continue to navigate this website beyond this page, cookies will be placed on yourbrowser. To learn more about cookies,
click here
.
Last, public administrations produce outputs and outcomes both of their own(such as a ministry of finance issuing debt), and to enable outputs and outcomesof frontline providers. The productivity of frontline, service delivery agencies suchas hospitals, schools, and police forces has been extensively measured, not leastas direct contact with citizens enables more direct measurement of servicedelivery outcomes (such as patient outcomes in hospitals or learning outcomes inschools) (see
chapter 29
).
This Handbook, instead, focuses on the analytics of administrative outputs andoutcomes. Administrative case data are one important source for measurementin such contexts. Such data are often routinely collected by organizations (forinstance, the number of tax or social security cases processed) and can berepurposed by organizations to measure outputs and outcomes (such as theamount of tax revenue raised), and thus gauge productivity (see
chapters 14
and
15
). Beyond administrative data, surveying households and citizens (such as byasking citizens about their trust in public administration organizations) can be animportant data source to understand outcomes of public administration (seechapter 28). What will be of most significance to measure and analyze willdepend on a specific organizational setting and topic of interest to decision-makers.
FIGURE 2.2 Mapping Different Government Analytics Data in the PublicAdministration Production Function
This site uses cookies to optimize functionality and give you the best possible experience.If you continue to navigate this website beyond this page, cookies will be placed on yourbrowser. To learn more about cookies,
click here
.
The various chapters of the Handbook provide insights into how to use thesedifferent data sources to understand how government is functioning andimprove the management of public administration. Table 1 below aims to helpyou identify which chapters might be of help to you, identifying which datasources and related analytics the chapters relate to. Try searching for key wordsof interest in the table search function.
This site uses cookies to optimize functionality and give you the best possible experience.If you continue to navigate this website beyond this page, cookies will be placed on yourbrowser. To learn more about cookies,
click here
.
Table 1. Administrative Data Sources for Government Analytics
in the Handbook
Search in table Page 1 of 2
Ch. #
Example uses of
analytics Example indicators
10 Examine fiscal planning
and sustainability of the
wage bill.
Wage bill by sector and
years
Allocate the workforce
across government
departments and
territories.
Distribution of civil
servants by rank
Set pay Turnover of civil
servants
Pay inequity between
Ministries
Retirement projections
11 Assess efficiency, equity
and effectiveness of
government spending
Budget execution rates
Assess whether
government spending
corresponds to budget
priorities
Share of government
expenditures covered
by FMIS
Identify large transactions
with high fiduciary risk
Share of total
expenditures by
transaction value
Overdue accounts
payable (aka payment
arrears)
12 Monitor procurement
markets and trends
Time needed for
contracting
Improve procurement and
contracting, e.g. by
identifying goods
organizations overpay
for, or organizations with
high corruption risks in
procurement
Number of bidders
Assess effects of distinct
procurement strategies or
reforms
Share of contracts with
single bidder
Final price paid for a
good or service
Share of contracts with
time overruns
Share of SME bidders
13 Assess the speed and
quality of administrative
back office processes
Adherence of
administrator's process
work to accepted
Data source
Payroll and HRMIS
Expenditure
Procurement
Administrative Process
This site uses cookies to optimize functionality and give you the best possible experience.
If you continue to navigate this website beyond this page, cookies will be placed on your
browser. To learn more about cookies, click here.
back-office processes
and process
implementation (for
instance for project
planning, budget
monitoring or
performance appraisals).
work to accepted
government procedure
Timeliness of
administrator's process
work with respect to
deadline
14 Assess customs revenue
collection
Time delays in customs
clearances
Assess trade facilitation
(flow of goods) through
customs across borders
Cost of customs
process
Assess whether customs
safeguards safety of
goods and protect people
(e.g. dangerous goods
not crossing)
Total customs revenue
collected
Number of goods in
infraction seized in
Customs
Surveys of public servants are an increasingly important tool for understanding the functioning of gover
Nonetheless, a common first choice concerns a survey mode: Are surveys conducted online, on paper, in
How can measures be designed that vary sufficiently, so that comparisons between organizations or gro
Finally, governments need to decide how to interpret and report results. For instance, can responses fro
This site uses cookies to optimize functionality and give you the best possible experience.If you continue to navigate this website beyond this page, cookies will be placed on yourbrowser. To learn more about cookies,
click here
.
News and Events
➜
Government Analytics Fellowship
➜
Government Analytics Handbook in Spanish (Abridged Version)
This site uses cookies to optimize functionality and give you the best possible experience.If you continue to navigate this website beyond this page, cookies will be placed on yourbrowser. To learn more about cookies,
click here
.
Download Overview
The Government Analytics Handbook: Leveraging Data to Strengthen Public ……
The Government Analytics Handbook
DOWNLOAD BOOK 
This site uses cookies to optimize functionality and give you the best possible experience.If you continue to navigate this website beyond this page, cookies will be placed on yourbrowser. To learn more about cookies,
click here
.
A Guide to Government Analytics
Chapters
Part 1: Overview
Chapter 1
:
The Power of Government Analytics to Improve Public Administration
Chapter 2
:
How to Do Government Analytics: Lessons From the Book
Chapter 3
:
Government Analytics of the Future
Part 2: Foundational Themes in Government Analytics
A collaboration between the Development Impact Evaluation Department, Office of the ChiefEconomist of Equitable Growth, Finance and Institutions.
SPONSORS
Handbook Teaser Trailer
This site uses cookies to optimize functionality and give you the best possible experience.If you continue to navigate this website beyond this page, cookies will be placed on yourbrowser. To learn more about cookies,
click here
.
Chapter 4
:
Measuring what matters: Principles for a balanced data suite that prioritizes problem-solvingand learning
Chapter 5
:
Practical Tools for Effective Measurement and Analytics
Chapter 6
:
The Ethics of Measurement of Public Administration
Chapter 7
:
Measuring and Encouraging Performance Information Use in Government
Chapter 8
:
Understanding Corruption Through Government Analytics
Part 3: Government Analytics Using Administrative Data
Chapter 9
:
Creating Data Infrastructures for Government Analytics
Chapter 10
:
Government Analytics Using Human Resource and Payroll Data
Chapter 11
:
Government Analytics Using Expenditure Data
Chapter 12
:
Government Analytics Using Procurement Data
Chapter 13
:
Government Analytics Using Data on the Quality of Processes
Chapter 14
:
Government Analytics Using Customs Data
Chapter 15
:
Government Analytics Using Administrative Case Data
Chapter 16
:
Government Analytics Using Machine Learning
Chapter 17
:
Government Analytics Using Data on Task and Project Completion
Part 4: Government Analytics Using Public Servant Surveys
Chapter 18
:
Surveys of Public Servants: The Global Landscape
Chapter 19
:
Determining Survey Modes and Response Rates: Do Public Servants Respond Differently toOnline and In-person Surveys?
Chapter 20
:
Determining Sample Sizes: How Many Public Officials Should Be Surveyed?
Chapter 21
:
Designing Survey Questionnaires: Which Survey Measures Vary, and for Whom?
Chapter 22
:
Designing Survey Questionnaires: To What Types of Survey Questions Do Public Servants NotRespond?
Chapter 23
:
Designing Survey Questionnaires: Should Surveys Ask about Public Servants' Perceptions ofTheir Organization or Their Individual Experience?
Chapter 24
:
Interpreting Survey Findings: Can Survey Results Be Compared across Organizations andCountries?
Chapter 25
:
Making the Most of Public Servants Survey Results: Lessons from Six Governments
Chapter 26
:
Using Survey Findings for Public Action: The Experience of the US Federal Government
Part 5: Government Analytics Using External Assessments
This site uses cookies to optimize functionality and give you the best possible experience.If you continue to navigate this website beyond this page, cookies will be placed on yourbrowser. To learn more about cookies,
click here
.
Chapter 27: Government Analytics Using Household Surveys
Chapter 28: Government Analytics Using Citizen Surveys: Lessons from the OECD Trust Survey
Chapter 29: Government Analytics Using Measures of Service Delivery
Chapter 30: Government Analytics Using Anthropological Methods
IBRD IDA IFC MIGA ICSID
Who We Are
News
Careers
Contact
Countries
Topics
Projects & Operations
Research & Publications
Events
Data
Knowledge Academy
Results Scorecard
This site uses cookies to optimize functionality and give you the best possible experience.
If you continue to navigate this website beyond this page, cookies will be placed on your
browser. To learn more about cookies, click here.
STAY CURRENT WITH OUR LATEST DATA & INSIGHTS
Sign Up
© 2025 World Bank Group. All Rights Reserved.
Legal Privacy Notice Site Accessibility Access to Information Scam Alert
Report Fraud or Corruption
==============================================================================================================================================================================================================
Data Analytics and Artificial
Intelligence
Project on
eProcurement System
(Government of Punjab)
Regional Training Institute, Jammu
Contents
Overview ................................................................................................................................................. 3
Objective ................................................................................................................................................. 4
Background ............................................................................................................................................. 4
About eProcurement .......................................................................................................................... 4
About Data Analytics and Artificial Intelligence ................................................................................. 5
How Machine Learning can help to analyse large databases such as the e-procurement database . 5
About the database ............................................................................................................................ 5
Participating Members............................................................................................................................ 6
Project Overview ..................................................................................................................................... 6
Algorithms used in the Project ........................................................................................................... 6
Technology Stack used for the implementation ................................................................................. 6
Key functions performed by the project ................................................................................................. 7
Functioned based on Data Analytics ................................................................................................... 7
1. Checking the validity of Pin codes and Permanent Account Number (PAN) in the dataset ... 7
2. Categorizing the tenders based on the tender validity period ............................................... 8
3. Finding tender-status wise number of tenders ...................................................................... 8
4. Finding the month-wise and year-wise unique work-items created in the database ............ 9
Functioned based on Machine Learning ........................................................................................... 10
1. Identifying the splitting of tenders ....................................................................................... 10
2. Clustering the tenders based on the differences between L1 & L2 and L1 & L3 .................. 12
3. Calculating the Risk Score based on the data available in ‘gep_tender_work_items’ and ‘gep_bids’ tables ........................................................................................................................... 12
Conclusion ............................................................................................................................................. 13
About the Accuracy of the modules ................................................................................................. 13
Areas for further improvement ........................................................................................................ 13
Recommendations for use in the Indian Audit & Accounts Department ............................................. 14
Appendix 1 – Analytic techniques/algorithms used in the project ....................................................... 15
K Medoids Clustering to cluster work items based on difference between L1 & L2 and L1 & L3 .... 15
Gaussian Mixture Model Clustering for checking correctness of the data....................................... 15
K-Means Clustering for checking the ranks of accepted AOC Bids and Evaluators Value ................ 16
Cosine Similarity Clustering for identifying the Splitting of tenders ................................................. 16
Appendix 2 – Dimensions identified to calculate the degree of similarity between the work items .. 17
Appendix 3 – Using the project ............................................................................................................. 18
Setting up of running environment .................................................................................................. 18
Running the project .......................................................................................................................... 18
Using different dataset in the project ............................................................................................... 19
Modifying the source code of the project ........................................................................................ 19
Banner image on the cover page taken from the website of eProcurement System, Government of Punjab (eproc.punjab.gov.in)
REGIONAL TRAINING INSTITUTE, JAMMU 3
Overview
An important tool for the development of the fullest potential of the people working in any organisation is the adoption of adaptive, topical and responsive training and capacity development mechanisms.
With a view to strengthen the practice –oriented research and project work undertaken in its Knowledge Centre, the Regional Training Institute (RTI), Jammu has introduced several innovative measures. Among these, is the conduct of a 9-week long internship programme for two 2nd year B. Tech students of the Indian Institute of Technology (IIT), Jammu as part of a collaborative alliance with the premier Institute.
The adoption of e-procurement mechanisms by various State Governments pose new challenges with regard to the conduct of a comprehensive and focused audit. The RTI team and the interns worked together on a project that sought to use Data Analytics and Artificial Intelligence to analyse the e-procurement system of the Government of Punjab and build systemic methods to query the transactions and bids in the database.
The resultant research, outlined in this paper being shared with the reader, showcases both the strengthening of audit methodologies and the enhanced ability to analyse large volumes of data to arrive at an audit conclusion, through the use of new technology.
While sharing the project, with all offices of the Indian Audit and Accounts Department (IA&AD), our team would like to thank the office of the Principal Accountant General (Audit), Punjab, for their cooperation in providing data and feedback whenever requested during the course of the research project.
The source code is available with the RTI Team and we would be happy to share the same with any sister office in the IAAD on request.
Any suggestions for improvement are welcome.
Jaya Bhagat
Jammu, 12th November 2020 Director General
REGIONAL TRAINING INSTITUTE, JAMMU 4
Objective
The objective of this project is to enhance the efficiency of audit teams by equipping them with an analytical tool that would help them to ascertain whether an adequate system is in place in the auditee units with regard to efficiency and transparency in public procurement. The project is intended for use by the external or internal auditors in an organization and is expected to make their job easier by way of scientific analyses of the process of handling public procurement by the Government as enhanced through automation and process re-engineering. The project should also be of help the auditors in determining as to whether the eProcurement system has facilitated greater clarity for the Government with regard to the overall picture of its procurement activities.
Background
About eProcurement
Public buying is an essential component of activities of major Government departments/ PSUs/ other bodies and authorities and therefore the procedure to be followed in public procurement must therefore conform to the yardsticks prescribed in the relevant clauses of the General Financial Rules, as enunciated in the Fundamental principles of public buying (for all procurements including procurement of works).
Every authority delegated with financial powers of procuring goods in the public interest shall have the responsibility and accountability to conduct such procurement keeping in view the tenets of efficiency, economy, and transparency, thereby ensuring the fair and equitable treatment of suppliers and promotion of competition in public procurement. In the present digital era, the mode of procurement to be followed is desirable through electronic means.
e-Procurement (electronic procurement, sometimes also known as supplier exchange) is the business-to-business or business-to-consumer or business-to-government purchase and sale of supplies, work, and services through the Internet as well as other information and networking systems, such as electronic data interchange and enterprise resource planning.
The e-procurement value chain consists of indent management, e-Informing, e-Tendering, e-Auctioning, vendor management, catalogue management, Purchase Order Integration, Order Status, Ship Notice, e-invoicing, e-payment, and contract management. Indent management is the workflow involved in the preparation of tenders. This part of the value chain is optional, with individual procuring departments defining their indenting process. In works procurement, administrative approval and technical sanction are obtained in electronic format. In goods procurement, indent generation activity is done online. The end result of this stage is taken as inputs for issuing the NIT.
Elements of e-procurement include request for information, request for proposal, request for quotation, RFx (the previous three together), and eRFx (software for managing RFx projects). With increased use of e-procurement, needs for standardization arise. Since the audit office is directly connected with scrutinising all the procedures for procurement, a need was felt by the RTI Team to prepare a module that would enable analysis of data relating to procurement procedures using Artificial Intelligence & data analytics. This module has been prepared in collaboration with IIT, Jammu, on the basis of data obtained from office of the PAG (Audit), Punjab.
REGIONAL TRAINING INSTITUTE, JAMMU 5
About Data Analytics and Artificial Intelligence
Data analytics is the science of analysing raw data in order to arrive at conclusions about that information. Data analytics techniques can reveal trends and metrics that would otherwise be lost in the mass of information. The scope of data analytics can be broad, from basic statistical models, business intelligence (BI), online analytical processing (OLAP) to various advanced analytics.
Artificial intelligence refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It focuses on the development of computer programs that can access data and use it learn for themselves.
How Machine Learning can help to analyse large databases such as the e-procurement database
With the digitization of most of the activities of the government departments, it becomes a necessity to upgrade the audit techniques as well. Use of data analytics to derive insights on a smaller dataset is generally sufficient because smaller datasets generally exhibit trends or the patterns that are simpler in nature and are easier to understand.
When it comes to working with large databases (such as the one used in this project), which hold data of multiple departments and further multiple offices and work items, data analytics may not succeed in detecting the correct patterns from the voluminous data. This is where the need for the use of Machine Learning (ML) is felt.
ML can help the auditor to obtain a general view the complete database and to detect patterns and anomalies in the database. It can also help the auditor to audit the complete population instead of a small sample thereby making the audit much more broad-based.
Since ML algorithms learn from the data that is analysed and can be programmed to learn from the user feedback, it can learn from the auditors and apply the learning to analyse data and point out anomalies that satisfy the conditions defined by the auditor.
A higher volume of data acts as an advantage when working with ML because it results in more training data being available to train the ML model. This helps the ML models to get trained for a varied range of scenarios which in turn, is reflected in more accurate results/predictions.
About the database
The database of the Punjab Government eProcurement System has been used in the project. The database was maintained as a PostgreSQL DB which is an open-source database. The database contained the tenders for all the major departments of the Punjab Government including the PWD, Agriculture, Food Civil Supplies and Consumer Affairs, Housing and Urban Development, Power, Local Government, Transport and Water Resources. The total size of the database was 4.3 GB and the database comprised 1034 tables (including the master tables). Based on preliminary analysis, the following tables were identified for being used for the project:
1. gep_tender_work_items – The table contained tender ID, work item reference number, tender title, work description, tender value, tender validity, tender inviting officer and other information pertaining to a tender.
2. gep_bids – The table contained tenderer ID, work item ID, bid reference ID, bid status, bid rank and other details of the bids placed through the eProcurement System.
3. gep_bid_aoc_details – The table contained bid ID and bid award value of the successful bids.
REGIONAL TRAINING INSTITUTE, JAMMU 6
4. gep_tenderer – The table contained the details about the firms and contractors registered in the eProcurement System.
5. gep_tender_basic_details – The table contained basic details about the tenders including the tender reference number and tender ID.
6. gep_orgchain_master – This was a master table that contained information about the departments that invite tenders through the eProcurement System.
7. gep_product_category – This was a master table that contained the types of the works and services and their associated IDs used in the eProcurement System.
Participating Members
1) The Indian Institute of Technology, Jammu
i) Ms. Arya, Pursuing B. Tech in Computer Science and Engineering (2018-2022)
ii) Ms. Unnam Pearly Susan, Pursuing B. Tech in Computer Science and Engineering (2018-2022)
2) The Regional Training Institute, Jammu
i) Ms. Jaya Bhagat, Director General (Overall guidance)
ii) Sh. J K Pandita, Sr AO (Training)
iii) Sh. Aseem Beetan, AAO (OIOS)
3) Office of the Principal Accountant General (Audit), Punjab
i) Ms. Monika, Sr Audit Officer (IT)
Project Overview
Algorithms used in the Project
The data available in the dataset was unlabelled. Therefore, the algorithms used in the project belong to the Unsupervised Learning domain which are used to draw inferences from the datasets consisting of data without labelled responses. The algorithms used in the project include K-Means clustering, K-Medoids clustering, Gaussian Mixture Model clustering and Cosine Similarity clustering. Details of the algorithms can be found in Appendix 1.
Technology Stack used for the implementation
Database – The original database of which the dump was provided by the office of the Principal Accountant General (Audit), Punjab was maintained as a PostgreSQL DB. Therefore, PostgreSQL Database was used to import the original data dump and for preliminary analysis of the data.
Programming Language – The Python programming language has been used for all the programming done in the project. Python is one of the most commonly used programming languages in the field of Data Analytics and Artificial Intelligence as it has a wide array of libraries to support AI related programming and it offers a high level of flexibility to build new models from scratch.
R programming language which offers a similar set of libraries and functionalities would also be suitable for use in the project as an alternate programming language.
Code Integration – Jupyter Notebook has been used for integrating all the code for the project. It is a web-based interactive computational environment for writing and executing code. In addition to working as an Integrated Development Environment, it can also be used to run and demonstrate the project.
REGIONAL TRAINING INSTITUTE, JAMMU 7
The following libraries of the Python Programming Language have been used for developing various functionalities of the project:
• Pandas – For manipulation of data
• Numpy – For working on large multi-dimensional arrays/matrices
• Datetime – To make computations on date and time
• Gradio – To create User Interface for the project
• Sklearn – To implement machine learning algorithms
• Matplotlib – For generating the visualizations
Key functions performed by the project
Functioned based on Data Analytics
This project demonstrates application of basic data analytics which can be used not only to point out discrepancies in the data but also the lack of controls in an IT System being used for the purpose of maintaining the data. The modules developed can perform the following functions:
1. Checking the validity of Pin codes and Permanent Account Number (PAN) in the dataset
Lack of data validation can lead to invalid entries in the databases. This module checks the data for the presence of the invalid Pin codes (in this instance, Pin codes that do not belong to the state of Punjab 1) and PAN. It also allows the user to export datapoints that contain invalid entries as a CSV file.
1 Based on the presumption that most procured items would be through in-state or local purchase.
Import the data file
Import the data file containing the work item containing the work item information information (gep_tender_work_items)(gep_tender_work_items)
Checking the
Checking the Pincodes present Pincodes present in the data in the data against the against the regular regular expression which expression which validates if the validates if the Pincode belongs Pincode belongs to the state of to the state of Punjab.Punjab.
If the Pincode is
If the Pincode is invalid, the invalid, the information information pertaining to the pertaining to the particular work particular work item is stored in a item is stored in a dataframe named dataframe named 'Pincode''Pincode'
The dataframe is
The dataframe is exported as a CSV file exported as a CSV file using the user using the user interfaceinterface
Import the data file
Import the data file containing the information containing the information pertaining to the bidders pertaining to the bidders (gep_tenderer)(gep_tenderer)
Checking the PAN
Checking the PAN present in the present in the data against the data against the regular regular expression which expression which validates if the validates if the PAN number is PAN number is valid or not.valid or not.
If the PAN is
If the PAN is invalid, the invalid, the information information pertaining to the pertaining to the bidder is stored in bidder is stored in a dataframe a dataframe named 'Pan'named 'Pan'
The dataframe is
The dataframe is exported as a CSV file exported as a CSV file using the user using the user interfaceinterface
REGIONAL TRAINING INSTITUTE, JAMMU 8
2. Categorizing the tenders based on the tender validity period
This module categorizes tenders into 9 categories according to the tender validity period. The user can apply the filters on the Year, Org Code2 and Org Chain.3
Multiple years can be selected at a time and the Org Chain chosen should have the selected Org code. A new column named “ValidityCategory” is added to the filtered data which provides the Category as per the tender validity. This newly formatted data can be exported as a CSV file. Also, a bar graph may be generated showing the number of works in each category from the filtered data.
3. Finding tender-status wise number of tenders
Tenders in the eProcurement database used in this project have been assigned ‘tender-status’ which is based on the status of the tender at a point of time.
2 Org Code refers to a unique alphabetical code used to identify each department in the database.
3 Org Chain outlines the path from the apex level of the Organization to the specific office which has floated the tender
Import the data file
Import the data file containing the work item containing the work item information information (gep_tender_work_items)(gep_tender_work_items)
Tender validity values of all the
Tender validity values of all the tenders are read and number of tenders are read and number of tenders falling in different ranges tenders falling in different ranges of tender validity are counted of tender validity are counted using nested 'elif' condition.using nested 'elif' condition.
Plotting of bar chart is done
Plotting of bar chart is done by seleting the Year, Org by seleting the Year, Org Code and Org Chain in the Code and Org Chain in the user interface.user interface.
REGIONAL TRAINING INSTITUTE, JAMMU 9
A tender can have any one of the following statuses: Published, Expired, Cancelled, Retender and Open. This module allows the user to see the number of tenders for each tender-status. Similar to the previous module, the user can apply filters on the Year, Org Code and Org Chain. The data satisfying the conditions entered can be exported as a CSV file.
A line plot showing the number of unique work items in each tender status is generated.
Also, irrespective of filters selected, the graph for the whole table is also displayed in the Jupyter notebook.
4. Finding the month-wise and year-wise unique work-items created in the database
Work items in the eProcurement database may sometimes have similar names and descriptions. Each work item has a unique work item ID associated with it which can be used to identify each work item. With the help of work item IDs, this module finds the number of unique work items created in each month of a year. The user selects the Year, Org Code and Org Chain using the user interface and has an option to export the filtered data as a CSV file.
Import the data file
Import the data file containing the work item containing the work item information information (gep_tender_work_items)(gep_tender_work_items)
Tender status (Published,
Tender status (Published, Expired, Cancelled, Retendered Expired, Cancelled, Retendered or Open) of all the tenders are or Open) of all the tenders are read and tenders are categorised read and tenders are categorised as per the status.as per the status.
Plotting of bar chart is done
Plotting of bar chart is done by seleting the Year, Org by seleting the Year, Org Code and Org Chain in the Code and Org Chain in the user interface.user interface.
Import the data file
Import the data file containing the work item containing the work item information information (gep_tender_work_items)(gep_tender_work_items)
Dates of creation of all the work
Dates of creation of all the work items are read; months and years items are read; months and years of creation of work items are of creation of work items are stored in a dataframe stored in a dataframe
Plotting of bar chart is done
Plotting of bar chart is done by seleting the Year, Org by seleting the Year, Org Code and Org Chain in the Code and Org Chain in the user interface.user interface.
REGIONAL TRAINING INSTITUTE, JAMMU 10
A bar graph showing the number of unique work items in each month of the selected year for the selected Org Chain is generated.
Functioned based on Machine Learning
This project further demonstrates the application of the Machine learning in the field of audit with the help of different modules that have been developed based on the audit criteria followed when auditing eProcurement activities of a government department. The modules developed using the Machine Learning algorithms can perform the following functions:
1. Identifying the splitting of tenders
The module computes the similarity between the work items based on multiple factors4 and therefore indicates the tenders which are likely to have been split from a single tender thereby helping the auditor to identify work items that need to be scrutinized in depth. This helps to avoid the problem of ‘salami slicing’ in procurement.
For extracting the details of tenders, the tender_basic_details and tender_work_items tables have been used. Out of the 54000 tenders, 27570 work items have corresponding entries in the tender_basic_details table. After merging the tables, all the null columns are dropped. The product categories in the dataset are represented by 143 numerical codes. These numbers are then replaced by their text counterparts using the master table gep_product_category. For the categorical data types, distance is calculated using the dice metric, where distance is considered ‘1’ when the values are unequal and ‘0’ otherwise. For the continuous text data types, the distance between the two texts is inversely proportional to similarity. The similarity is calculated using the Cosine Similarity method. For the continuous real values, the distance is the absolute difference between the two values divided
4 Appendix 2 - Dimensions identified to calculate the degree of similarity between the work items
Import data from
Import data from gep_tender_basic_detail, gep_tender_basic_detail, gep_tender_work_items, gep_tender_work_items, gep_orchain_master and gep_orchain_master and gep_product_categorygep_product_category
Selection of
Selection of paramters and paramters and creation of creation of dataframe dataframe using the using the selected selected parametersparameters55
Dropping of
Dropping of redundant redundant datadata
Replacing the
Replacing the product product category code category code with the with the name of the name of the product product categorycategory
Calculation of
Calculation of distance distance between work between work items and items and creation of creation of multidimensional multidimensional array to store the array to store the distances.distances.
Selection of work item in
Selection of work item in user interface and user interface and plotting of bar chart plotting of bar chart showing its degree of showing its degree of similarity with other similarity with other workwork--items from the items from the database.database.
REGIONAL TRAINING INSTITUTE, JAMMU 11
by their range. For the continuous DateTime values, the number of days between the days divided by the range of days from a beginning date (for dataset used, 17th June 2010).
The distance between two tenders is therefore calculated using the individual distance metrics for different types of columns. For each column, the distance between them is calculated and multiplied with weights assigned to each column based on their impact on the splitting results and added. Using this method, for any tender selected by the user, the twenty most similar tenders are identified using the distance calculation and exported to a CSV file.
The source code can be modified to generate a list containing more than 20 work items. However, in the instant case, the most similar work items that may exhibit a case of ‘salami slicing’ usually form part of the first 10-15 items and remaining items may exhibit a low degree of similarity with the selected work item.
The details of the 20 work items are also generated in a tabular form which can be exported as .csv file for further analysis.
Bar charts depicting the ‘degree of similarity of the predicted work items and the selected work item’ and ‘month-wise number of similar works issued’ are produced for the user.
REGIONAL TRAINING INSTITUTE, JAMMU 12
2. Clustering the tenders based on the differences between L1 & L2 and L1 & L3
Differences between the L1, L2 and L3 generally indicate the degree of competition in the tendering process. A substantial difference between the lowest bid values can sometimes be due to collusion in tendering by the bidders. The absolute differences in the bid values may vary widely depending on the nature and value of the bid. Therefore, the module clusters the tenders based on the relative differences in the bid values with the view to find out the tenders for which the differences are unrealistically high.
It calculates and assigns a risk score to the work items based on these differences. Tenders with high risk scores can subsequently be looked into in detail by the auditor as part of a risk-based audit.
3. Calculating the Risk Score based on the data available in ‘gep_tender_work_items’ and ‘gep_bids’ tables
This module uses the data available in in the ‘gep_tender_work_items’ and ‘gep_bids’ to train an ML model and cluster the work items as per the risk score which is calculated based on factors such as tender value, tender validity, product category, number of bids received, L1, L2 and L3 values, differences between L1 and L3 values and whether retendering was done if a single bid was received for the work item. Thereafter the users can enter the hypothetical bid details to calculate a risk score for those bids. The risk score helps the auditors to identify tenders/work items that need to scrutinized. This module can be used by the auditors to calculate the risk score of any bid for which basic information is available.
Import data from
Import data from gep_tender_workgep_tender_work_items and _items and gep_bids tablesgep_bids tables
Calculation of
Calculation of differences differences between L1 & between L1 & L2 and L1 & L3 L2 and L1 & L3 values of the values of the work itemswork items
Clustering the
Clustering the work items by Kwork items by K--Means Means Clustering as the Clustering as the data is data is numerical in numerical in naturenature
Thecluster
Thecluster information of information of work items work items pertaining to a pertaining to a particular auditee particular auditee unit can be viewed unit can be viewed by the auditor by the auditor using the user using the user interface.interface.
Import data
Import data from from gep_tender_wgep_tender_work_items and ork_items and gep_bids gep_bids tablestables
Calculation of
Calculation of risk score based risk score based on tender value, on tender value, tender validity, tender validity, product product category, category, number of bids number of bids received and received and whether single whether single or multiple bids or multiple bids were received were received for a work item.for a work item.
Clustering the
Clustering the work items by work items by KK--Means Means Clustering as Clustering as the data is the data is numerical in numerical in naturenature
Thecluster
Thecluster information of work information of work items pertaining to a items pertaining to a particular auditee particular auditee unit can be viewed unit can be viewed by the auditor using by the auditor using the user interface.the user interface.
REGIONAL TRAINING INSTITUTE, JAMMU 13
In a situation when required information is not available for a few options, the user can input an approximate value to calculate the risk score.
Conclusion
About the Accuracy of the modules
The implemented Machine Learning models will perform well even for the updated database as long as data of a similar nature is added to the database and the models are retrained at regular intervals. However, if an altogether a new dataset which is not at all similar to the data present in the database is used for training the models, the models may not perform well because the project has been developed based on the structure and type of data that was available in the eProcurement database of the Government. The project can be made to work for the database of a similar structure by gradually training the model using the new data.
The results of the Data Analytics and Machine Learning models also depend on the data present in the database. Presence of validation checks in the system ensures that there are no invalid entries in the dataset and hence improves the results of the analysis. Data cleaning techniques are applied to remove erroneous/invalid and duplicate entries from the dataset before training a model. If the dataset does not have significantly high number of invalid entries, data cleaning helps to refine the dataset which leads to accurate results by the model. However, if the number of erroneous/invalid entries is high, as in case of the eProcurement database, valuable data is lost during the data cleaning phase. This loss of data resulted in lesser data and insights for the model to learn on. The data validation methods can substantially help in creating very large and authentic datasets that will result in the algorithms continuously refining and learning more in league with the human knowledge in the Audit Domain.
Areas for further improvement
Currently, the modules use Cosine Similarity and Jaccard Similarity while comparing the texts. These methods work with mathematical transformations on the text, instead of their meaning. These methods, therefore, cannot identify synonyms and understand the text with the nuances of the English language. Natural language processing algorithms can be more effective in such cases but they could not be used for this project they require a large corpus of documents related to procurement and tendering to train the models and this was not available.
REGIONAL TRAINING INSTITUTE, JAMMU 14
The current risk analysis works on unsupervised algorithms. With labelled data in place indicating which tenders have a high risk and are more prone to be erroneous, supervised algorithms can be used, which have the potential to learn how to categorise the tenders. These models try to mimic human behaviour and try to apply this in terms of mathematical algorithms and formulae.
Recommendations for use in the Indian Audit & Accounts Department
The modules developed as part of this project can be used in audit for the functions described above. It is to be noted that the results given by the models are generally based on a pattern or a trend and cannot be expected to be hundred percent accurate. However, the results generated will help the auditors to identify the risk areas more efficiently and enable them to spend their time and effort during an audit in a focussed manner.
REGIONAL TRAINING INSTITUTE, JAMMU 15
Appendix 1 – Analytic techniques/algorithms used in the project
K Medoids Clustering to cluster work items based on difference between L1 & L2 and L1 & L3 Clustering is a Machine Learning technique that involves the grouping of data points. Given a set of data points, we can use a clustering algorithm to classify each data point into a specific group. In theory, data points in the same group should have similar properties and features, while data points in different groups have profoundly different properties and features.
The default metric for calculating the distance between each of the points while training the model is the Euclidean distance. In comparison, the results are useful for datasets with continuous variables, but the metric fails to consider the concept behind the categorical values. For example, while computing the distance between two data points having Contract Type value as either 1, 2 or, 3, where one is Tender, two is Rate Contract, and three is Empanelment. Euclidean distance considers the difference between 1 and 3 to be more than 1 and 2, where due to the data’s categorical nature, one is as distant from two as three. Therefore, the Dice distance metric is used for computing the distances between categorical points. Since the dataset is mixed, we use a distance metric called Gower Distance. Gower metric incorporates the Dice distance for the categorical columns and Manhattan distance (absolute difference between the values) for the continuous columns. The clustering algorithm used for this module is K-Medoids, (also called as Partitioning Around Medoid). A medoid is a point in the cluster whose dissimilarities with all the other points in the group are minimum. In K-Means clustering which is commonly used method of clustering numerical data, the representative point of a cluster is the mean of all points within a cluster. The representative point of a cluster in K-Medoid clustering is a datapoint inside that cluster which is comparable to the median which is robust and preferable to use when the data is categorical in nature. Let the number of clusters be ‘k.’ Then the algorithm initializes ‘k’ random medoids out of the data-set and assigns clusters to all the points in the dataset. The cost is calculated using the dissimilarity scores between medoids and data points. After this, a random point in the cluster is chosen to be the medoid. If the cost increases this way, the swap is undone. The optimal number of clusters is decided by calculating the algorithm’s performance on different values, ideally ranging from two to eight. For this project, the inertia values took a steep decrease (Elbow Method) when the number of clusters was 6. Thus, the dataset was fitted on the K-Medoids algorithm with six number of clusters.
Gaussian Mixture Model Clustering for checking correctness of the data
(By checking the ranges of awarded values of accepted AOC Bids)
The awarded values in the database are in a similar range. So to generate meaningful clusters, Gaussian Mixture Model Clustering is used because the clusters formed from this clustering are stretched and can be helpful in the cases where all the data points are very similar.
To find the number of clusters to be used, the Bayesian Information Criterion (BIC) Score was used; the lower the value, the simpler the model, the better the performance. Usually the BIC Score decreases with an increasing number of clusters. Therefore, for the computational purposes, it was decided to have five as the number of clusters.
Bids were clustered based on the reasonability of the bid values and probability of a datapoint (bid value) lying in each cluster is computed using sklearn libraries in-built functionality predict probability.
REGIONAL TRAINING INSTITUTE, JAMMU 16
K-Means Clustering for checking the ranks of accepted AOC Bids and Evaluators Value
The data values used for this model are bid ranks and evaluator’s value. The evaluator’s value was not recorded in a few cases, and these empty places were replaced by 0. The bid ranks contain textual data that cannot be clustered, so it was changed into numerical values. The bid ranks which were not recorded were given the rank Nil. To change the bid ranks into numerical values, the rank similarity was computed with L1, L2, L3, H1, H2, and H3. The Jaccard Similarity method was used on the ranks. The Jaccard similarity can be calculated as
Jaccard Similarity = 𝐴∩𝐵𝐴∪𝐵
Where A, B are two words, and the set operations are done on the letters of the word. The Jaccard Similarity was calculated for both L1,L2,L3, and H1, H2,H3 and the maximum of these two was considered. After finding out the percentage of similarity, if the percentage is less than 100, it is replaced by 0 to get proper clusters. The data was normalized and the model was fit based on the Jaccard Similarity and evaluators’ value. K-Means clustering algorithm was selected for this model. The number of clusters for this algorithm was determined by the silhouette score, a measure of how similar an object is to its cluster compared to other groups. The cluster number was decided to be 6 based on the silhouette score and the efficiency of the model.
Cosine Similarity Clustering for identifying the Splitting of tenders
Cosine similarity measures the similarity between two datapoints of multiple dimensions. Based on the similarity the data points are clustered such that the work items that seem very similar as per the dimensions defined while development would fall together in a cluster.
Fourteen major dimensions5 were identified and the similarity between the work items was calculated based on those fourteen dimensions and the weightage assigned to each of these dimensions using the Cosine Similarity method. For the categorical data types, distance is calculated using the dice metric, where distance is considered ‘1’ when the values are unequal and ‘0’ otherwise. For the continuous text data types, the distance between the two texts is inversely proportional to similarity. For the continuous real values, the distance is the absolute difference between the two values divided by their range to normalize them. For the continuous DateTime values, the number of days between the days divided by the range of days from a beginning date (for this dataset, 17th June 2010).
The distance between two tenders is therefore calculated using the individual distance metrics for different types of table columns. For each column, the distance between them is calculated and multiplied with weights assigned to each column based on their impact on the splitting results and added. Using this method, for any chosen tender, the ten most similar tenders are identified using the distance calculation and exported to a CSV file. The graphs for the year-wise and month-wise distribution of these ten most similar tenders are also created. Another bar graph to plot the ten work items (work item ID) against their similarity index. The similarity index is reciprocal of 1+distance between the tenders.
5 Appendix 2 - Dimensions identified to calculate the degree of similarity between the work items
REGIONAL TRAINING INSTITUTE, JAMMU 17
Appendix 2 – Dimensions identified to calculate the degree of similarity between the work items
1) Categorical dimensions
i) createdby - User ID that floated the tender on the E-procurement platform
ii) actualorgid - Organisation that floated the tender
iii) contracttype - The type of contract, Tender, Rate Contract, or Empanelment
iv) pincodid – Pin code of the location of the tender
v) tendercategoryid - The superset of product categories (can be 1, 2, or 3)
2) Continuous (String) dimensions
i) tendertitle - The tender title
ii) workdesc - A brief description of the tender floated
iii) bidopeningplace - The bid opening place
iv) productcategory - The product category, for example, Civil Works - Roads
v) location – Location of work
vi) invitingofficer - The designation of the officer floating the tender
vii) invitingofficeraddress - The office address of the officer who invited the tender
3) Continuous (Float/Real) dimensions
i) tendervalue - The final value of the tender
4) Continuous (Datetime / Timestamp) dimensions
i) createddate – The date when the tender was created in the eProcurement System
REGIONAL TRAINING INSTITUTE, JAMMU 18
Appendix 3 – Using the project
Setting up of running environment
Following are the software requirements for running the project on a computer:
• Python programming language (including Integrated Development Environment for Python -IDLE) - The complete project has been developed in the Python programming language that is platform independent and thus can be installed on both Windows and Linux Operating Systems. Python can be downloaded from here.
• Jupyter Notebook – It is an open-source web application which can be used to create and share documents that contain live code, equations, visualizations and narrative text. Jupyter Notebook can be installed in a computer having Python preinstalled. It can be installed by following the instructions given here.
• The following libraries of Python which have been used in the project need to be installed in the computer:
o Pandas
o Numpy
o Sklearn
o Random
o Datetime
o Matplotlib
o Gradio
o Scipy
o Seaborn
Libraries can be installed by running the command ‘pip install <library name>’ (without quotes) in the commandline interface. Detailed information on installing/upgrading libraries can be found here.
Running the project
The source code of the project is available with the Regional Training Institute, Jammu. The institute may be contacted for requesting sharing of the same. The project can then be run by following the steps given below:
• Place the source code and the tables (.csv files) in the same folder on the computer on which Python, Jupyter Notebook and the required libraries are installed.
• Open Jupyter Notebook by running the command ‘jupyter notebook’ (without quotes) in the commandline interface.
• The jupyter notebook being a web application, will open in the default browser.
• In the jupyter notebook, browser to the location where the source code and the csv files have been placed and open the Python code file with the extension ‘.ipynb’
• The code would be loaded in the jupyter notebook in some time.
• The notebook consists of code snippets, comments and visualizations of the project.
• The code snippets can be executed by selecting the code snippet and pressing ‘Shift + Enter’ keys on the keyboard or by clicking the ‘Run’ button in the interface at the top.
REGIONAL TRAINING INSTITUTE, JAMMU 19
• The complete code in the notebook can be executed by clicking on highlighted button.
• The functions performed by the project can be accessed with the help of Gradio user interface in the notebook as shown under ‘Key functions performed by the project’.
Using different dataset in the project
The project will perform well if a different dataset which is similar in structure to the original dataset is used in the project. A different dataset can be used in the project by changing the contents of the csv files to be used in the project while keeping the structure and data types of the columns unchanged. After any change in the data is made, the complete notebook should be run again in order to retrain the models.
Modifying the source code of the project
The source code can be easily modified as per the requirements of the user to add new functionalities or to improve the existing functionalities of project. After the modification has been made, all the succeeding code snippets must be executed again to get the desired results.
======================================================================================================================================================================================================================================================================================================================
CHAPTER 17
 Government Analytics 
Using Data on Task and 
Project Completion
 Imran Rasul, Daniel Rogger, Martin Williams, and  
Eleanor Florence Woodhouse
 SUMMARY
 Much government work consists of the completion of tasks, from creating major reports to under
taking training programs and procuring and building infrastructure. This chapter surveys a range of 
methods for measuring and analyzing task completion as a measure of the performance of government 
organizations, giving examples of where these methods have been implemented in practice. We discuss 
the strengths and limitations of each approach from the perspectives both of practice and research. 
While no single measure of task completion provides a holistic performance metric, when used appro
priately, such measures can provide a powerful set of insights for analysts and managers alike.
 ANALYTICS IN PRACTICE
 ● Much government activity can be conceived as discrete tasks: bounded pieces of work with definite 
outputs. Public sector planning is often organized around the achievement of specific thresholds; the 
completion of planning, strategy, or budgetary documents; or the delivery of infrastructure projects. 
Task completion is a useful conception of government activity because it allows analysts to assess public 
performance in a standardized way across organizations and types of activity.
 ● Assessing government performance based solely on the passing of legislation or the delivery of frontline 
services misses a substantial component of government work. Using a task completion approach pushes 
analysts to better encapsulate the breadth of work undertaken by public administration across govern
ment. It thus pushes analysts to engage with the full set of government tasks.
 Imran Rasul is a professor in the Department of Economics, University College London. Daniel Rogger is a senior economist in the 
World Bank’s Development Impact Evaluation (DIME) Department. Martin Williams is an associate professor in the Blavatnik School 
of Government, University of Oxford. Eleanor Florence Woodhouse is an assistant professor in the Department of Political Science and 
School of Public Policy, University College London. 
365
● A task completion approach also allows for the investigation of which units and organizations are 
most likely to initiate, make progress on, and complete tasks. Though not a full picture of government 
work—it is complementary to the analysis of process quality or sector-specific measures of quality, for 
example—it allows for a rigorous approach to comparisons frequently made implicitly in budgetary and 
management decisions.
 ● Collecting data across projects on determinants of progress, such as overruns, and matching them to 
input data, such as budget disbursements, allows for a coherent investigation of the mechanisms driving 
task progress across government or within specific settings.
 ● Attempting to assess task completion in a consistent way across government is complicated by the fact 
that tasks vary in nature, size, and complexity. By collecting data on these features of a task, analysts can 
go some way toward alleviating concerns over the variability of the tasks being considered. For example, 
analysis can be undertaken within particular types of task or size, and complexity can be conditioned on 
in any analysis. An important distinction in the existing literature is how to integrate the analysis of tasks 
related to the creation of physical infrastructure and tasks related to administration.
 INTRODUCTION
 A fundamental question for government scholars and practitioners alike is whether governments are per
forming their functions well. What these functions are and what performing “well” means in practice are 
complex issues in the public sector, given the diverse tasks undertaken and their often indeterminate nature. 
Despite the importance of these questions, there is little consensus as to how to define government effec
tiveness in a coherent way across the public service or how to measure it within a unified approach across 
governments’ diverse task environments (Rainey 2009; Talbot 2010). Such considerations have practical 
importance because government entities, such as political oversight or central budget authorities, frequently 
have to make implicit comparisons between the relative functioning of public agencies. For example, when 
drawing up a budget, public sector managers must make some comparison of the likely use of funds across 
units and whether these funds will eventually result in the intended outputs of those units, however varied 
the tasks are in scope. From an analytical perspective, the more comprehensive a measure of government 
functioning, the greater the capacity of analytical methods to draw insights from the best-performing parts 
of government.
 Much government activity can be conceived as discrete tasks: bounded pieces of work with definite out
puts. Public sector planning is often organized around the achievement of specific thresholds; the comple
tion of planning, strategy, or budgetary documents; or the delivery of projects. Government projects are also 
often conceived as bounded activities with definite outputs but frequently encompass multiple tasks within 
a wider conception of completion. Task completion (or project completion) is thus a useful conception of 
government activity—however that activity is conceived—because it allows analysts to assess public perfor
mance in a standardized way across organizations and types of activity. This kind of assessment contrasts 
with continuous regulatory monitoring, the assessment of the stability of citizens’ access to frontline services, 
and the equity of activities related to redistribution, which are better understood as the evaluation of ongoing 
processes. In this chapter, we propose a way to leverage data on task completion to assess the government’s 
effectiveness across its diverse task environments and learn from related analysis. We argue that by utilizing 
a unified framework for task completion, analysts can assess whether a government or government agency 
“does well what it is supposed to do, whether people. . . work hard and well, whether the actions and proce
dures of the agency and its members help achieve its mission, and, in the end, whether it actually achieves its 
mission” (Lee and Whitford 2009, 251; paraphrasing Rainey and Steinbauer 1999).
 Task comparison is useful for three core reasons. First, task completion is a concept that can be applied 
across much government work, thus allowing for a broad consideration of government functioning. 
366
 THE GOVERNMENT ANALYTICS HANDBOOK
We believe that by working with a task completion framework, analysts can gain a fuller and more accu
rate picture of the functions of government that reflects the full range of government activities—from 
human resource management to policy definition, infrastructure planning and implementation, service 
delivery, and audit and evaluation. We know little about the full distribution of tasks that public admin
istrators undertake. As shown in figure 17.1, the few studies that do apply a task completion framework 
f
 ind that administrators undertake a vast range of activities—from advocacy to auditing and monitoring to 
planning—that go well beyond infrastructure and service delivery, the activities that are usually considered 
in the academic literature.
 Figure 17.1 displays the frequency of the most prevalent tasks undertaken by Ghanaian public officials 
in their daily duties. Infrastructure provision for the public (rather than upgrading government facilities) is 
the most common activity, partly motivating our particular attention to it in this chapter. However, the figure 
indicates the broad diversity of tasks undertaken by the public service. The distinct colors in each bar of the 
histogram indicate different organizations undertaking that type of task. Thus, it is clear that each type of 
task is undertaken by many different organizations. Second, a task completion framework pushes analysts 
to think carefully about the characteristics of each of the tasks assessed. We define projects above as collec
tions of tasks; an obvious question is how to apply boundaries to tasks or projects uniformly across govern
ment. There is very limited research on the characteristics of the tasks undertaken by public administrators 
and how to assess whether they are being undertaken adequately. The task completion framework pushes 
analysts to think in detail about the activities that administrators engage in and how successfully they do so. 
T
 hat is to say, they must think not only about whether a bridge is completed but what the full conception of 
the bridge project is, whether the bridge was of a complex design that was hard to implement, whether the 
quality of the implementation of the bridge is adequate, whether it was completed within a reasonable time 
frame given the complexity of the project, and so on.
 FIGURE 17.1 Task Types across Organizations
 1,000
 800
 Number of tasks
 600
 400
 200
 0
 Source: Rasul, Rogger, and Williams 2021 .
 Note: The task type classification refers to the primary classification for each output . Each color in a column represents an organization 
implementing tasks of that type, but the same color across columns may represent multiple organizations . Figures represent all 
30 organizations with coded task data . ICT = information and communication technology .
 Infrastructure—public
 Advocacy
 Monitoring, review, and audit
 Training
 Policy development
 Personnel management
 Procurement
 Research
 Permits and regulation
 Financial and budget management
 Infrastructure—o ces
 367
 CHAPTER 17: GOVERNMENT ANALYTICS USING DATA ON TASK AND PROJECT COMPLETION
 ICT management
T
 hird, by creating comparators from across government, an integrated measurement approach yields ana
lytical benefits that more than make up for the losses from abstraction for many types of analysis. Analysts can 
investigate the determinants of successful task completion from a large sample, with varying management envi
ronments, buffeted by differential shocks, and on which a greater range of statistical methods can be effectively 
applied. The task completion framework has the advantage of capturing a wide range of activities and being 
comparable across departments. Thus, the framework can pool task types to allow analysts to draw conclusions 
about government effectiveness more broadly, rather than, for example, allowing only for inferences about a 
specific type of task (for example, the delivery of a specific service, such as passport processing times, versus 
a range of government tasks that are indicative of administrative effectiveness more broadly). By leveraging 
these data—on the nature, size, and complexity of tasks—analyses can be undertaken within particular types 
of tasks—for example, distinguishing between the completion of physical and nonphysical outputs. Taking the 
analysis a step further, having measures of task completion that are comparable across teams or organizations 
can enable researchers to identify the determinants of task completion—although this entails its own method
ological challenges and is beyond the scope of this measurement-focused chapter.
 Much of the existing literature that seeks to describe how well governments perform their functions 
focuses on upstream steps in the public sector production process (such as budgetary inputs or processes), 
which are useful for management (but not so relevant to the public) (Andrews et al. 2005; Lewis 2007; 
Nistotskaya and Cingolani 2016; Rauch and Evans 2000). Or it focuses on final outcomes (such as goods 
provided or services delivered), which are relevant to the public (but not always so useful for management) 
(Ammons 2014; Boyne 2003; Carter, Klein, and Day 1992; Hefetz and Warner 2012). The completion of tasks 
and projects falls between these two approaches: it is useful for management and relevant to the public. The 
task completion framework helps analysts address the gap between inputs and final outcomes in terms of 
how they measure government performance. It gives analysts a way to engage with the full distribution of 
government tasks and to assess the characteristics of the tasks themselves. The task approach outlined in this 
chapter is closely aligned with the discussion in chapter 15 of The Government Analytics Handbook. There, 
the relevant task is the processing of an administrative case. Clearly, there are other types of tasks in govern
ment, and this chapter aims to present a framework that can encapsulate them all. Given the scale of case 
processing in government, however, chapter 15 presents a discussion specific to that type of task. Related 
arguments can be made for chapter 12 on procurement, chapter 14 on customs, and chapter 29 on indicators 
of service delivery. Across these chapters, the Handbook provides discussions of the specific analytical oppor
tunities afforded by different types of government activity. These chapters contain some common elements, 
such as discussions of some form of complexity in relation to the task under focus. This chapter showcases 
the considerations required for an integrated approach across task types.
 T
 hrough the task completion framework, we aim to encourage practitioners and scholars alike to con
ceive of government activity more broadly and to leverage widely available data sources, such as government 
progress reports or independent public expenditure reviews, to do so. As well as being widely available, these 
kinds of objective data are highly valuable because they usually cover a wide range of different task types 
performed by numerous different agencies and departments.
 T
 his chapter continues as follows. First, we conceptualize government work as task completion. Second, we 
use tasks related to the creation of physical infrastructure to illustrate the task completion framework. Third, we 
show how the framework applies to other types of tasks. Fourth, we explore how to measure task characteristics 
(considering the complexity of tasks and their ex ante and ex post clarity). Fifth, we discuss key challenges in 
integrating these measures with each other and into management practice. Finally, we conclude.
 CONCEPTUALIZING GOVERNMENT WORK AS THE COMPLETION OF TASKS
 Much of the literature in public administration has focused on how to measure government effectiveness 
by relying on the tasks of single agencies (Brown and Coulter 1983; Ho and Cho 2017; Lu 2016), on a set of 
368
 THE GOVERNMENT ANALYTICS HANDBOOK
agencies undertaking the same task (Fenizia 2022), or on a broad conception of the central government as 
a single entity (Lee and Whitford 2009). These approaches limit analysis to a single conception of govern
ment effectiveness, which in the case of a single agency or sector, can be precisely defined. However, almost 
by definition, this limits analysis to a subset of government work and thus raises concerns over what such 
analysis tells us about government performance as a whole or how performance in one area of government 
affects other areas.
 In addition to measuring government effectiveness on the basis of a partial vision of government, many 
studies that have sought to investigate government effectiveness have relied on perception-based measures of 
effectiveness, based on the evaluations of either government employees or external stakeholders and experts 
(Poister and Streib 1999; Thomas, Poister, and Ertas 2009; Walker et al. 2018). Such measures are frequently 
available only at an aggregate or even country level because of how distant these individuals are from actual 
government tasks, and they frequently assess not the outputs of those tasks directly but some perception of 
“general effectiveness.”1
 Objective measures of government functioning have frequently been eschewed because of obstacles 
related to data availability, their purported inability to capture the complexity of government work, or 
conflicting understandings of what effectiveness means. However, many government agencies produce their 
own reports on the progress they have made across the full distribution of their work. Similarly, agencies 
often have administrative data on the totality of their activities that provide quantities related to the com
plexity of task completion that can be repurposed for analytics. These data are collected for management and 
reporting purposes as part of the daily duties of agency staff. These reports frequently contain characteristics 
of the tasks undertaken and progress indicators outlining how far tasks have progressed. These reports can 
be the basis of an integrated analysis of government functioning.
 For tasks related to physical infrastructure and administration, analysts can use quantities from these 
reports, or similar primary data collection, to conceptualize government work in a unified task completion 
framework. The following discussion of the strengths and limitations or challenges of such an approach 
focuses on a small set of research papers that have applied a task completion framework to the assessment of 
government functioning. It thus aims to illustrate the utility of the task completion framework rather than 
being in any way comprehensive. Where relevant, we provide a number of examples of how public officials 
have taken a similar approach.
 We rely on two simple definitions throughout the chapter. First, a task is the bounded activity for which 
a given organization, team, or individual in the government is responsible. Second, an output is the final 
product a government organization, team, or individual delivers to society. An output is the result of a suc
cessful task. In government performance assessment, outputs are defined as “the goods or services produced 
by government agencies (e.g., teaching hours delivered, welfare benefits assessed and paid).”2 An example of 
a government task might be developing a draft competition policy or organizing a stakeholder meeting to 
validate the draft competition policy (Rasul, Rogger, and Williams 2021, appendix). The corresponding out
puts would be the draft competition policy itself and the holding of the stakeholder meeting. These tasks are 
usually repeated and are completed within varying time frames, depending on the complexity and urgency 
of the activity at hand.
 More granular guidance on how to define a task is challenged by the fact that the appropriate conception 
of a task will vary by the focus of the analysis. However, to illustrate common conceptions, some examples 
from the analyses that will be discussed in this chapter include the design, drilling, and development of a 
water well (including all taps linked to a single source of water); the design, construction, and finishing of a 
school; the renovation of a neighborhood sewage system; a full maintenance review and associated activities, 
such as resurfacing, to bring a road up to a functioning state as determined by local standards; the develop
ment of a new public health curriculum for primary school students; and the updating of a human resources 
management information system with current personnel characteristics for all health-related agencies.
 By conceiving all government activity as consisting of tasks with intended outputs, analysts can con
struct a standardized measure of government performance and can gather multiple tasks together to 
assess  government performance across teams within an organization, across organizations, and over time. 
369
 CHAPTER 17: GOVERNMENT ANALYTICS USING DATA ON TASK AND PROJECT COMPLETION
Government performance can be defined as the frequency with which particular government actors are able 
to produce outputs from corresponding tasks. We now turn to considerations in the definition of a task or 
project and an output in the case of physical and nonphysical outputs.
 Physical Outputs
 We first consider a task completion framework as it pertains to the accomplishment of physical 
infrastructure, or, more precisely, tasks relating to the production of physical outputs. In lower-middle- 
income  countries in particular, the noncompletion of infrastructure projects is a widespread and costly 
phenomenon, with recent estimates suggesting that over one-third of the infrastructure projects started in 
these countries are not completed (Rasul and Rogger 2018; Williams 2017).
 We focus on task completion measures developed from coding administrative data that are at least some
what comparable across organizations and can be implemented at scale, rather than on performance audits 
of specific programs (for example, by national audit offices or international financial institutions’ internal 
performance reports) or on the evaluation of performance against key performance indicators (for example, 
in leadership performance contracts or through central target-setting mechanisms). Many governments or 
government agencies have infrastructure-project-tracking databases (either electronic or in paper-based files). 
T
 hese records may be for implementation management, for budgeting and fiduciary reasons, or for audit and 
evaluation. These databases keep records of how far physical projects have been implemented relative to their 
planned scope.
 For example, in Nigeria, Rasul and Rogger (2018) use independent engineering assessments of thou
sands of projects from across the government implemented by the Nigerian public service to assess the 
functioning of government agencies. They complement this with a management survey in the agencies 
responsible for the projects and examine how management practices matter for the completion rates of 
projects. The analysis exploits a specific period in the Nigerian public service when “the activities of public 
bureaucracies were subject to detailed and independent scrutiny” (2) and a special office was set up to track 
the quality of the project implementation of a broad subset of government activities. This was due to an effort 
by the presidency to independently verify the status of many of the public infrastructure projects funded by 
the proceeds of debt relief and implemented by agencies across the federal government. The records of this 
tracking initiative allowed the authors to quantify both the extent of project implementation and the assess
ment of the quality of the public goods provided.
 A second application of the task completion framework to an empirical setting examining physical 
outputs is provided by Williams (2017), who collects, digitizes, and codes district annual progress reports in 
Ghana. These reports, which are written annually by each district’s bureaucracy and submitted to the central 
government, include a table listing basic information about projects that were ongoing or active during the 
calendar year. Such reports are widely produced but not frequently available in a digital format or used for 
government analytics. The potential of these data for useful insights into government performance is great. 
Williams uses the reports on physical projects to examine the determinants of noncompletion, presenting 
evidence that corruption and clientelism are not to blame but rather a dynamic collective action process 
among political actors facing commitment problems in contexts of limited resources.
 Similarly, Bancalari (2022) uses district administrative data on sewerage projects in Peru to explore the 
social costs of unfinished projects. She uses a combination of mortality statistics, viability studies, annual 
budget reports on sewerage projects (which allow her to identify unfinished and completed projects), spatial 
topography data, and population data in order to provide evidence that infant mortality and under-five 
mortality increase with increases in unfinished sewerage projects. She also finds that mayors who are better 
connected to the national parliament are able to complete more projects.
 Beyond using administrative data, analysts have also undertaken primary fieldwork to explore the com
pletion of physical projects. For example, Olken (2007) uses various surveys on villages, households, individ
uals, and the assessments of engineering experts to investigate the level of corruption involved in building 
roads in Indonesia. Olken is able to produce a measure of corruption in terms of missing expenditures by 
370
 THE GOVERNMENT ANALYTICS HANDBOOK
calculating discrepancies between official project costs and an independent engineer’s estimate of costs 
defined by the survey responses. Primary field activity also allows analysts to undertake randomized con
trolled trials of potential policies to improve government functioning. In the case of Olken (2007), random
ized audits of villages are used to estimate the effect of top-down monitoring on the quality of government 
outputs: in this case, the building of roads. Such a research design and measure are highly valuable and 
capture a very important feature of government activity, although they come at a high cost in terms of the 
resources needed to capture these government tasks.
 Other papers have studied the maintenance rather than the construction of physical outputs. In these cases, 
task completion is the effective continuation of physical outputs. Once again using primary fieldwork to collect 
required data, Khwaja (2009) uses survey team site visits and household surveys to measure the maintenance 
of infrastructure projects in rural communities in northern Pakistan (Baltistan) as a form of task completion. 
Maintenance here is measured through surveys of expert engineers who assess the maintenance of infrastruc
ture projects in terms of their physical state (that is, how they compare to their initial condition), their func
tional state (that is, the percentage of the initial project purpose satisfied), and their maintenance-work state 
(that is, the percentage of required maintenance that needs to be carried out). Khwaja (2009) uses these data to 
examine whether project design can improve collective success in maintaining local infrastructure. The paper 
presents within-community evidence that project design makes a difference to maintenance levels: “designing 
projects that face fewer appropriation risks through better leadership and lower complexity, eliciting greater 
local information through the involvement of community members in project decisions, investing in simpler 
and existing projects, ensuring a more equitable distribution of project returns, and emulating NGOs can 
substantially improve project performance even in communities with low social capital” (Khwaja 2009, 913).
 We have seen several examples of “government analytics” that seek to measure the completion rate of 
tasks related to the provision (or maintenance) of physical outputs. From Nigerian federally approved social 
sector projects, such as providing dams, boreholes, and roads, to Indonesian road building, analysts have 
defined measures of task completion based on physical outputs. The analysis has used administrative data, 
existing household surveys, and primary fieldwork (sometimes in combination with one another) to gener
ate insights into the determinants of government functioning.
 T
 hese papers measure task completion in a series of different ways that all aim to capture the underlying 
phenomenon of what share of the intended outputs are completed. But there are important commonalities to 
their approaches. First, the definition of a task or project is determined by a common, or consensus, engi
neering judgment that crosses institutional boundaries. Thus, though a ministry of urban development may 
bundle the creation of multiple water distribution points, the building of a health center, and road repaving 
into a single “slum upgrading” project, the analysts discussed above split these groupings into individual 
components that would be recognizable across settings, and thus across government. A water distribution 
point will be conceived as a discrete task whether it is a component of a project in an agriculture, education, 
health, or water infrastructure project. The wider point is that an external conception of what makes up a 
discrete activity, such as the common engineering conception of a water distribution point, provides disci
pline on the boundaries of what is conceived as a single task for any analytical exercise.
 Second, within these conceptions of projects, an externally valid notion of completion and progress can be 
applied. For example, the threshold for a water distribution point is that it produces a sufficient flow of water 
over a sustained period for it to be considered “completed.” Williams (2017) uses the engineering assessments 
included in administrative data to categorize projects into bins of “complete” (for values such as “complete” or 
“installed and in use”) or “incomplete” (for values such as “ongoing” or “lintel level”). Rasul and Rogger (2018) 
use engineering documents specific to each project to define a percentage scale of completion for each project 
allowing for a more granular measure of task progress, mapping them along a 0–1 continuum. Thus, highly 
varied project designs are mapped into a common scale of progress by consideration of the underlying produc
tion function for that class of infrastructure. What constitutes a halfway point in the development of a water 
distribution point and a dam will differ, but both can be feasibly assessed as having a halfway point.
 T
 hird, notions of scale or complexity can be determined from project documentation, providing a basis 
for improving the credibility of comparisons across tasks. As will be discussed in section three, there is 
little consensus about how to proxy such complexity across tasks. The literature on complexity in project 
371
 CHAPTER 17: GOVERNMENT ANALYTICS USING DATA ON TASK AND PROJECT COMPLETION
management and engineering emphasizes the multiple dimensions of complexity (Remington and Pollack 
2007). This can be seen as a strength, in that a common framework for coding complexity can be flexibly 
adapted to the particular environment or analytical question. In the above examples, planned (rather than 
expended) budget is frequently used as one way to proxy scale and complexity. The challenge is that the 
planned budget may already be determined by features related to task completion. For example, the history 
of task completion at an agency may influence contemporary budget allocations.
 For this reason, physical infrastructure tasks can be conceptualized and judged by external conceptions 
and scales that discipline the analysis. A strength of these measurement options is that they offer a relatively 
clear, unambiguous measure of task completion. Fundamentally, generating a sensible binary completion 
value requires understanding how progress maps onto public benefit (for example, an 80 percent finished 
water distribution point is of zero public value). With this basic knowledge across project types, task comple
tion indicators can be computed for the full range of physical outputs produced by government.3
 However, this type of task completion framework measurement also comes with limitations. It is easier 
to measure completion than quality with these types of measures. Quality is typically multifaceted, such that 
it is more demanding to collect and harmonize into an indicator that can be applied across project types. In 
Rasul and Rogger (2018), assessors evaluate the quality of infrastructure projects on a coarse scale related to 
broad indicators that implementation is of “satisfactory” quality relative to professional engineering norms. 
Analysis can then be defined by whether tasks are, first, completed, and second, completed to a satisfactory 
level of quality. Administrative progress reports vary in their information content but tend to assume quality 
and focus on the technical fulfillment of different stages in the completion process.
 One way to gain information on quality is to undertake independent audits or checks, though these tend 
to be highly resource intensive relative to the use of administrative data. For example, Olken (2007, 203) 
relies on a team of engineers and surveyors to assess the quality of road infrastructure, who “after the proj
ects were completed, dug core samples in each road to estimate the quantity of materials used, surveyed local 
suppliers to estimate prices, and interviewed villagers to determine the wages paid on the project.” From 
these data, Olken constructs an independent estimate of the quality of each road project.
 Some conceptions of quality go as far as the citizen experience of the good or service or how durable or 
well managed it is. Rasul and Rogger (2018) also include assessments of citizen satisfaction with the project 
overall as determined by civil society assessors, but such data are almost never available in administrative 
records and have to be collected independently.
 T
 here are also issues pertaining to the reliability and interpretation of task completion that are worth 
highlighting. First, doubts may be raised when the progress reports that act as the foundation for task com
pletion assessments are provided by the same public organizations that undertake the projects themselves 
(see the discussion in chapter 4). For this reason, they may not constitute reliable measures of progress, or 
at least may be perceived as unreliable. The problem is whether organizations can be considered reliable in 
their assessments of their own work. Measures of task progress sourced from administrative data must thus 
be used with care and, ideally, validated against a separate (independent) measure of progress. A good exam
ple of this comes from Rasul, Rogger, and Williams (2021), who match a subsample of tasks from govern
ment-produced progress reports to task audits conducted by external auditors in a separate process.4 Such 
validation exercises can be very helpful in providing evidence that the measures produced by government 
organizations on their own performance are credible, thus salvaging an important source of data that might 
otherwise be deemed unusable.
 Additionally, noncompletion can mean different things depending on how the timeline of infrastruc
ture procurement, construction, and operation is organized. This is especially clear in the case described by 
Bancalari (2022), where it is hard to establish whether the effect uncovered is an effect of noncompletion 
or delays and cost overruns in delivery.5 It can be hard to distinguish noncompletion (a project will remain 
unfinished) from delays (a project will be completed but is running over schedule). Here, the point in time 
when one decides to measure completion and the initial time frame set for a given task become important 
and can affect how one interprets task noncompletion.
 Finally, a separate issue pertains to whether tasks are completed as planned, not simply whether they are 
completed. The existing literature from management studies has mostly focused on overruns, delays, and 
372
 THE GOVERNMENT ANALYTICS HANDBOOK
over-estimated benefits rather than on noncompletion per se (Bertelli, Mele, and Whitford 2020; Post 2014). 
T
 his body of literature tends to focus on the service and goods delivery side of government rather than on the 
full range of government activities. However, it is an important complement to the task completion framework 
precisely because it focuses on whether the tasks governments undertake are being completed and are being 
completed in the time frame and up to the standard that they were planned for. For example, a vast body of lit
erature emphasizes the value-for-money or cost calculations of infrastructure projects rather than the efficiency 
or effectiveness of the processes via which they are delivered (for example, Engel, Fischer, and Galetovic 2013). 
Scholars such as Flyvbjerg (2009, 344) have argued that the “worst” infrastructure gets built because “ex ante esti
mates of costs and benefits are often very different from actual ex post costs and benefits. For large infrastructure 
projects the consequences are cost overruns, benefit shortfalls, and the systematic underestimation of risks.”
 Nonphysical Outputs
 Now we turn to the task completion framework as it applies to the production of nonphysical outputs. Exam
ples of nonphysical outputs are auditing activities, identifying localities where infrastructure is required, 
raising awareness about a given social benefit scheme, or planning for management meetings. These types of 
task, in short, involve government activities that pertain to the less visible side of government: not delivery 
in the form of physical goods or services but the planning, monitoring, information sharing, reviewing, and 
organizational tasks of government.
 Rasul, Rogger, and Williams (2021) use administrative data on the roughly 3,600 tasks that civil ser
vants undertook in the Ghanaian civil service in 2015. The data on these tasks are extracted from quarterly 
progress reports and represent the full spectrum of government activities. As can be seen from figure 17.1, 
a large proportion of these tasks are related to nonphysical outputs. For each type of task, in relation to both 
physical and nonphysical outputs, the researchers identify a scheme by which to judge task completion by 
allocating a threshold of progress to represent completion for each task type.
 Rasul, Rogger, and Williams (2021) also collect data on the management practices under which these tasks 
are undertaken via in-person surveys with managers covering six dimensions of management: roles, flexibility, 
incentives, monitoring, staffing, and targets. Together, the task and management data allow for an assessment of 
how public sector management impacts task completion, allowing for the comparison of the effect of man
agement practices on the same tasks across different organizations. Their data demonstrate, first, that there 
is substantial variation in task completion across types of task and across civil service organizations. Second, 
there is also substantial variation in the types of management practice that public servants are subject to across 
organizations, and the nature of management correlates significantly with task completion rates.
 Integrating the analysis of tasks related to both physical and nonphysical outputs allows for a broad 
assessment of government functioning, encompassing the many interactions between tasks of different 
natures. Such a holistic approach also enables the assessment of tasks with different underlying characteris
tics, which has long been identified as a core determinant of government performance.
 Rasul, Rogger, and Williams (2021) are interested in exploring whether different management tech
niques are differentially effective, depending on the clarity of the task in project documents. They build on 
the literature arguing that where settings involve intensive multitasking, coordination, or instability, man
agement techniques using monitoring and incentive systems are likely to backfire. The question, as they put 
it, harking back to the Friedrich vs. Finer debate (Finer 1941; Friedrich 1940), is “to what extent should [civil 
servants] be managed with the carrot and the stick, and to what extent should they be empowered with the 
discretion associated with other professions?” (Rasul, Rogger, and Williams 2021, 262). Their central finding 
is that there are “positive conditional associations between task completion and organizational practices 
related to autonomy and discretion, but negative conditional associations with management practices related 
to incentives and monitoring” (Rasul, Rogger, and Williams 2021, 274).6 The authors distinguish between 
government tasks with high and low ex ante and ex post clarity. Incentives and monitoring-intensive man
agement approaches are hypothesized (and found) to be more effective when ex ante task clarity is high 
373
 CHAPTER 17: GOVERNMENT ANALYTICS USING DATA ON TASK AND PROJECT COMPLETION
(and ex post task clarity is low), whereas autonomy and discretion-intensive management approaches are 
relatively more effective when ex ante task clarity is low (and ex post task clarity is high).
 T
 he main contribution of Rasul, Rogger, and Williams (2021) to the discussion of this chapter is provid
ing a holistic, output-based organizational performance metric. However, their approach also takes a holistic 
account of the multifarious nature of management practices in government and showcases the value of com
bining such data. The authors “conceptualize management in public organizations as a portfolio of practices 
that correspond to different aspects of management, each of which may be implemented more or less well. 
Bureaucracies may differ in their intended management styles, that is, what bundle of management practices 
they are aiming to implement, and may also differ in how well they are executing these practices” (262). That 
is, there is a combination of both intent and implementation when it comes to management practices that 
may affect the effectiveness of an organization. The task completion framework, with its focus on both the 
breadth of activities that government bodies undertake and on the detail of the characteristics of government 
tasks, represents an important stepping stone toward a more holistic and realistic understanding of govern
ment work and effectiveness.
 A separate body of literature that brings together tasks and projects of distinct types into a single analyt
ical framework is the literature on donor projects. For example, using data on the development projects of 
international development organizations (IDOs)—specifically, eight agencies—including project outcome 
ratings of holistic project performance, Honig (2019) investigates the success of IDO projects according to 
internal administrative evaluations. The success ratings are undertaken by IDO administrators, who employ 
a consistent underlying construct across different IDOs, with an OECD-wide standard in place. These rat
ings are combined with a host of other variables capturing various features of the projects (for example, their 
start and end dates, whether there was an IDO office presence in situ, what the sector of the project is, etc.).
 Honig (2019, 172, 196) uses “variation in recipient-country environments as a source of exogenous 
variation in the net effects of tight principal control” to find that “less politically constrained IDOs see 
systematically lower performance declines in more unpredictable contexts than do their more-constrained 
peers.” That is to say that monitoring comes with costs in terms of reducing the ability of agents to adapt, 
particularly in less predictable environments.
 Similarly, Denizer, Kaufmann, and Kraay (2013, 288) leverage a data set of over 6,000 World Bank proj
ects (over 130 developing countries) to “simultaneously investigate the relative importance of country-level 
‘macro’ factors and project-level ‘micro’ factors in driving project level outcomes.” The authors leverage 
Implementation Status Results Reports completed by task team leaders at the World Bank, which report on 
the status of the projects, as well as Implementation Completion Reports, which include a “subjective assess
ment of the degree to which the project was successful in meeting its development objective” (290), plus 
more detailed ex post evaluations of about 25 percent of projects, in order to assess project outcomes. They 
f
 ind that roughly 80 percent of the variation in project outputs occurs across projects within countries, rather 
than between countries, and that a large set of project-level variables influence aid project outputs.
 A related but separate body of literature considers nonphysical task completion by frontline delivery 
agents. For example, using the case of the Department of Health in Pakistan, Khan (2021) undertakes an 
experiment in which he randomly emphasizes the department’s public health mission to community health 
workers, provides performance-linked financial incentives, or does both. He measures task completion 
through a combination of internal administrative data on service delivery and outputs, gathered as part 
of routine monitoring processes, and household surveys of beneficiaries. Mansoor, Genicot, and Mansuri 
(2021), instead, use the case of the agriculture extension department in Punjab, Pakistan, to measure both 
objective task completion and supervisors’ subjective perception of performance. They measure this through 
a combination of household surveys and data from a mobile phone tracking app that frontline providers use 
to guide and record their work.
 Analogous to the physical outputs case, then, to apply a task completion framework to tasks related to 
nonphysical outputs, we require common definitions of tasks that cross institutional boundaries, exter
nally valid notions of completion and progress, and notions of scale or complexity. Such external stan
dards for what completion and quality look like across institutions are rare, but they do exist in some 
f
 ields, such as health care (see the example of the joint health inspection checklist in Bedoya, Das, and 
374
 THE GOVERNMENT ANALYTICS HANDBOOK
Dolinger [forthcoming]). Creating an analogous approach to these issues for tasks related to nonphysical 
outputs ensures comparability with tasks related to physical outputs. However, they are also valid pillars for 
analysis even within the set of tasks related to nonphysical outputs only.
 For many tasks related to nonphysical outputs, there are, in fact, natural conceptions of task and output. 
For example, a curriculum development project is only complete once the curriculum is signed off on by 
all stakeholders, and an infrastructure monitoring program is only complete when a census of the relevant 
infrastructure has been completed. Similarly, such an approach can be developed for measures of progress. 
T
 he curriculum development will typically be broken down into substantive stages in planning documents, 
and each of these stages can be assigned a proportion of progress. In the infrastructure monitoring case, a 
simple proportion of infrastructure projects assessed, perhaps weighted by scale or distance measures, seems 
f
 itting. Not all cases will be so clear-cut. To identify a consensus definition of task by task type that could 
apply across institutional boundaries, Rasul, Rogger, and Williams (2021) employ public servants at a central 
analytics office (in the Ghanaian case, this was the Management Services Department) to agree on relevant 
definitions using data from across government. As will be seen below, this team also defines measures of 
complexity relevant across the full set of tasks, including (as mentioned above) clarity of design. Decisions 
as to how to define task completion will be influenced by, but then very much influence, the approach to 
data collection. Table 17.1 summarizes the approaches analysts have taken to measuring task completion for 
physical and nonphysical outputs.
 While we have focused our discussion mainly on research-oriented examples of measuring task comple
tion, there are also examples of government organizations’ use of task completion measures for tasks related 
to physical and nonphysical outputs—with varying degrees of formality. For example, the United Kingdom 
Infrastructure and Projects Authority conducts in-depth annual monitoring of all large-scale projects across 
UK government departments—235 as of 2022—and publishes an annual report with a red/amber/green 
project outlook rating (IPA 2022). At the other end of the formality and resource-intensiveness spectrum, in 
their engagement with the government of Ghana in 2015–16 in the course of conducting fieldwork, Rasul, 
Rogger, and Williams (2021) found that Ghana’s Environmental Protection Agency tallied the percentage of 
outputs completed by each unit in their quarterly and annual reports for internal monitoring purposes. In 
between these two examples, the Uganda Ministry of Finance and the International Growth Centre (IGC) 
have partnered to apply Rasul, Rogger, and Williams’s (2021) coding methodology (supplemented with 
qualitative interviews) to monitor the implementation progress of 153 priority policy actions across govern
ment and examine the determinants of their completion (Kaddu, Aguilera, and Carson n.d.). And of course, 
as argued above, many if not most government organizations do some form of task or output completion 
measurement in the course of their own routine reporting—despite most not taking the next step of using 
these data for formal analytical purposes.
 TABLE 17.1 Selected Measures of Task Completion
 Task type
 Physical tasks 
Potential data sources and measurement methods
 ● Site visits by expert teams
 ● Site visits by survey teams
 ● Compilation from other secondary sources 
(for  example, media or project reports)
 ● Administrative data from periodic reports
 Nonphysical tasks ● Surveys of beneficiaries or citizens
 ● Tracking app used by frontline personnel
 Selected examples
 Olken (2007); Rasul and Rogger (2018)
 Khwaja (2009)
 Flyvbjerg, Skamris Holm, and Buhl (2002); 
Williams (2017)
 Bancalari (2022)
 ● Administrative data from periodic reports
 ● Administrative data from internal  management 
monitoring sources
 ● International donor project evaluation reports
 Source: Original table for this publication .
 Khan (2021) 
Mansoor, Genicot, and Mansuri (2021) 
Rasul, Rogger, and Williams (2021) 
Mansoor, Genicot, and Mansuri (2021), 
Khan (2021)
 Denizer, Kaufmann, and Kraay (2013), 
Honig (2019)
 375
 CHAPTER 17: GOVERNMENT ANALYTICS USING DATA ON TASK AND PROJECT COMPLETION
Applying the task completion framework to nonphysical outputs comes with its challenges and lim
itations, building on those noted above for physical outputs. The issues pertaining to assessing the quality 
of the implementation of tasks related to nonphysical outputs are twofold. First, establishing how to assess 
quality is not straightforward, and second, the nature of a task can render the difficulty of assessing quality 
differentially complex. For instance, if the task one is measuring is the completion of a bridge, one first has to 
establish the criteria that dictate whether it can be considered a high- or low-quality bridge, whereas if one 
is also considering nonphysical outputs, such as the development of an education strategy, then one faces a 
potentially even greater challenge in defining what “high-quality” means for such a project (see Bertelli et al. 
[2021] for a discussion of this).
 T
 here are certain types of task, in short, for which establishing objective benchmarks is more difficult 
than for others. It does not seem like too much of a leap, for example, to hypothesize that the nonphysical 
tasks we have considered in this section might frequently be more complex to benchmark in terms of quality 
than the physical outputs we described earlier.
 T
 his difficulty creates discontinuity in measurement quality across physical and nonphysical goods, 
which, in turn, raises the issue of the potential endogeneity of task and output selection. That is to say, out of 
the universe of possible government tasks, the types of tasks we are best able to measure may be correlated 
with particular outputs. This could provide us with a distorted image of the types of tasks that are conducive 
to producing certain outputs.
 MEASURING TASK CHARACTERISTICS
 As we outline in the introduction to this chapter, a task completion framework is helpful to analysts in two 
main senses. First, it pushes analysts to better encapsulate the breadth of work undertaken by public admin
istration across government. Second, it encourages them to think carefully about the characteristics of the 
tasks themselves. In this section, we will focus on the latter feature of a task completion framework: how to 
measure task characteristics.
 T
 here are, naturally, a plethora of government task characteristics on which one could focus. Here, we 
will focus on several of the most relevant characteristics from the perspective of implementation. We con
centrate on implementation because it has been the focus of the literature on task completion and because it 
is of direct relevance to the work of practitioners, the intended audience of this chapter.
 We start by considering task complexity. When examining government outputs and their relationship to 
phenomena such as management practices, government turnover, or risk environment, it is often import
ant to understand their relationship with project or task complexity (Prendergast 2002). This is because the 
complexity of the task will frequently be strongly correlated with variables such as time to completion, total 
cost, the likelihood of delays, and customer satisfaction, which might be of interest to scholars or practi
tioners interested in task completion. Table 17.2 summarizes how the analysts described in this paper have 
attempted to implement measurement of complexity, as well as how authors have measured two further 
important features of government tasks to which we will turn next, visibility and clarity.
 Rasul and Rogger (2018, 12), in their study of public services in the Nigerian civil service, create 
complexity indicators that capture “the number of inputs and methods needed for the project, the ease 
with which the relevant labour and capital inputs can be obtained, ambiguities in design and project 
implementation, and the overall difficulty in managing the project.” They are thus able to condition on the 
complexity of projects along these margins when exploring the relationship between managerial practices 
and project completion rates. However, such an approach does not account for the fact that worse-per
forming agencies may be assigned easier (less complex) tasks in a dynamic process over time. So in 
background work for the study, Rasul and Rogger assess the extent to which there was sorting of projects 
across agencies by their level of complexity, a task only feasible with appropriate measures. They do not 
f
 ind any evidence of such sorting.
 376
 THE GOVERNMENT ANALYTICS HANDBOOK
TABLE 17.2 Selected Measures of Task Characteristics
 Task or project 
characteristic
 Complexity 
Visibility 
Clarity  
(ex ante and ex post) 
Potential data sources and measurement methods
 ● Expert data coding from site visits
 ● Semi-expert data coding from administrative 
reports
 ● International donor project evaluation reports
 Selected examples
 Khwaja (2009); Rasul and Rogger (2018); 
Rasul, Rogger, and Williams (2021); 
Denizer, Kaufmann, and Kraay (2013)
 ● Project-level data from infrastructure database 
assembled from governmental and financial sources
 ● Semi-expert data coding from administrative 
reports 
Source: Original table for this publication .
 Woodhouse (2022)
 Rasul, Rogger, and Williams (2021)
 Khwaja (2009, 915), instead, captures project complexity by creating an index that measures whether 
“the project has greater cash (for outside labor and materials) versus noncash (local labor and materials) 
maintenance requirements, . . . the community has had little experience with such a project, and . . . the proj
ect requires greater skilled labor or spare parts relative to unskilled labor for project maintenance.” In this 
way, he is able to distinguish group-specific features—such as social capital—from features of task design—
 such as degree of complexity—in order to better understand their relative importance to one another.
 Denizer, Kaufmann, and Kraay (2013) also consider complexity in their study of how micro (project- 
level) or macro (country-level) factors are correlated with aid project performance, albeit as a secondary 
focus. Using three proxies for project complexity (the extent to which a project spans multiple sectors, a 
project’s novelty, and the size of the project), they find “only some evidence that larger—and so possibly more 
complex—projects are less likely to be successful. On the other hand, greater dispersion of a project across 
sectors is in fact significantly associated with better project outcomes, and whether a project is a ‘repeater’ 
project or not does not seem to matter much for outcomes” (Denizer, Kaufmann, and Kraay 2013, 302).
 Given, then, that the issue of accounting for complexity is widespread and often relies upon assessments 
that are not anchored to an external concept or measure of what complexity is, what are some of the ways 
that analysts can validate their measures of complexity? Rasul, Rogger, and Williams (2021), in their con
struction of a measure of the complexity of the tasks being undertaken by Ghanaian civil servants, ensure 
that coding is undertaken by two independent coders because the variables they measure require coders to 
make judgment calls about the information reported by government agencies. They also implement rec
onciliation by managers in cases where there are differences between coders. Discussion between coders 
and managers about how they see different categories or levels of complexity can be a good way to iron out 
differences in the measurement of complexity.
 Another way to ensure consistency in measuring complexity can be to randomly reinsert particular tasks 
into the set of tasks being assessed by the coders to check whether they award the same complexity score to 
identical tasks. This is something that Rasul and Rogger (2018) do in their construction of a measure of task 
complexity completed by the Nigerian civil service. Rasul and Rogger (2018) also assess the similarity of 
scores between their two coders and leverage the passing of time to get one of the coders to recode a subsam
ple of projects from scratch (without prompting) to assess the consistency of coding in an additional way.
 In a similar spirit, audits of coding can be an effective way to validate a measure of complexity, albeit 
a costly one. For example, Rasul, Rogger, and Williams (2021, 265) use an auditing technique to check the 
validity of their measure of task completion; they “matched a subsample of 14% of tasks from progress 
reports to task audits conducted by external auditors through a separate exercise.” Although this technique 
was applied to task completion, a similar method could easily be used to validate a complexity measure in 
many contexts; if there are data available on the technical complexity of a task (for example, from engineers 
or other field specialists), such assessments could be used to check a subsample of the analyst’s own evalu
ations of complexity. Rasul and Rogger (2018), for example, work with a pair of Nigerian engineers to get 
them to assess the complexity of government tasks according to five dimensions.
 377
 CHAPTER 17: GOVERNMENT ANALYTICS USING DATA ON TASK AND PROJECT COMPLETION
Another salient feature of government tasks is how easy it is to define a given task and to evaluate 
whether and when it has been completed. This feature is related to, but conceptually separate from, the 
complexity of the task. Rasul, Rogger, and Williams (2021) call this feature ex ante and expost task clarity. 
According to their definition, bureaucratic tasks are “ex ante clear when the task can be defined in such a way 
as to create little uncertainty about what is required to complete the task, and are ex post clear when a report 
of the actual action undertaken leaves little uncertainty about whether the task was effectively completed” 
(Rasul, Rogger, and Williams 2021, 260).
 Task clarity is an important characteristic to consider, especially in relation to management practices, 
because the types of management strategy that one wishes to implement may be heavily influenced by 
the types of task that they govern. Indeed, Rasul, Rogger, and Williams (2021, 260) hypothesize, and find 
evidence, that “top-down control strategies of incentives and monitoring should be relatively more effec
tive when tasks are easy to define ex ante because it is easier to specify what should be done and construct 
an appropriate monitoring scheme.” On the other hand, they also theorize (and, again, find evidence) that 
“empowering staff with autonomy and discretion should be relatively more effective when tasks are unclear 
ex ante, as well as when the actual achievement of the task is clear ex post” (260).
 T
 he clarity of task definition is thus also important to take into consideration when exploring questions 
pertaining to the management of public administration. The degree to which a task is easy to describe and 
evaluate has a significant bearing on the types of management strategy that make sense to employ when 
undertaking that task. Task clarity can also impact a number of other features of government work, such as 
the level of political and citizen support it enjoys—with simpler, more visible projects tending to garner more 
interest from politicians and support from citizens (Mani and Mukand 2007; Woodhouse 2022)—or the 
degree to which a task is subject to measurement or performance-pay mechanisms.
 Task clarity is important to measure for its potential interactions with the concepts of effort substitution 
and gaming (Kelman and Friedman 2009). If performance measures are applied only to those tasks that are 
ex ante and ex post clear, such tasks may be prioritized to the detriment of others because they are subject to 
measurement or because bureaucrats seek to “game” the system by focusing their attention on improving sta
tistics relating to their performance but not their actual performance. As we have seen in the work of Honig 
(2019) and Khan (2021), it is especially in complex, multidimensional task environments where granting 
autonomy or discretion to bureaucrats can have beneficial results. In short, thinking about the nature of the 
task at hand and its interaction with features such as the management practices being adopted and individual 
behavioral responses on the part of public servants and politicians is highly important if one wants to get to 
the bottom of “what works” in government.
 DISCUSSION: KEY CHALLENGES
 T
 he previous sections have reviewed the scattered and relatively young literature on the systematic mea
surement of task and project completion in government organizations. The measurement methods and data 
sources identified hold great promise for practitioners and researchers but also present a number of concep
tual and practical challenges. While we have discussed some of these above in relation to specific papers or 
measurement methods, in this section, we briefly highlight some cross-cutting issues for measurement and 
analysis as well as for integration into management practice and decision-making.
 T
 he first challenge is determining what a task is. At the beginning of this chapter, we defined outputs as 
the final products delivered by government organizations to society and tasks as the intermediate steps taken 
by individuals or teams within government to produce those outputs. We characterized both as discrete, 
bounded, and clearly linked to each other. While this is conceptually useful and can serve as a guide for mea
surement, it is also a profound simplification of the messy, interlinked, and uncertain reality of work inside 
most government organizations. Indeed, the research insights produced by several of the studies we have 
discussed emphasize that the ambiguity, complexity, and interconnection of tasks and bureaucratic actions 
378
 THE GOVERNMENT ANALYTICS HANDBOOK
often mean that simplistic management efforts do not produce their anticipated effects. Analysts interested 
in measuring task completion must thus strike a difficult balance between identifying distinct tasks, projects, 
and outputs in order to measure their completion and simultaneously calibrating their analysis and inference 
to capture the nuances of the effective performance of these tasks.
 A second and related challenge is drawing appropriate inferences from measures of task completion, 
which, in itself, is just a descriptive fact of the level of task performance. On its own, measuring task com
pletion does not diagnose the causes of task (in)completion, predict future levels of performance, pinpoint 
needs for improvement, or measure the performance of the individual personnel responsible for a task 
(since factors outside their control may also matter). It does, however, provide a foundation upon which to 
conduct further analysis along these lines. Indeed, for most of the studies cited above, the measurement of 
task completion simply provides a dependent variable for analysis of a diverse range of potential factors and 
mechanisms. This chapter has focused mainly on the measurement of this dependent variable; linking it to 
causes and consequences requires additional analysis, which will differ in its aims and methods depending 
on an analyst’s purposes.
 A third challenge relates to integrating the measurement of task completion into practice and man
agement—that is, taking action based on it. One main challenge relates to the well-known potential for 
gaming and distorting effort across multiple tasks (Dixit 2002; Propper and Wilson 2003), exemplified by 
“Goodhart’s Law”: “any observed statistical regularity will tend to collapse once pressure is placed upon it for 
control purposes” (Goodhart 1984). In other words, it may well be possible to accurately measure task com
pletion in government organizations, but using these measures for the purpose of management—particularly 
if it involves benefits or consequences for the actors involved—risks undermining the validity of the mea
sures and their linkage to bureaucratic performance. See the discussion in chapter 4. While some strategies 
can be put in place to mitigate such effects (for example, data quality audits or measuring multiple dimen
sions of bureaucratic performance), these are nearly always imperfect. Analysts should thus seek to innovate 
in measuring task completion as a means of improving understanding while being cautious and selective in 
how they use it to guide management actions.
 A final consideration in deciding what tasks to measure and how is the trade-off between prioritizing 
breadth and comparability, on the one hand, and specificity and depth, on the other. Figure 17.2 illustrates 
this trade-off. In general, task completion measures that are widely applicable across the whole of govern
ment will naturally tend to be less specific to (and hence less informative of) the performance of any given 
unit or task. An example of this might be the type of data contained in a government’s annual report, budget 
execution report, or multiyear plan, which usually cover the whole of government activity but do so at a rel
atively shallow level. At the other extreme, researchers or practitioners can gather a great deal of information 
about the completion of a specific task, as a performance audit might do. This gives a very informative pic
ture of the completion of that particular task but permits little comparison across tasks or units. In between, 
one can locate the various measurement options we have discussed in this chapter. For example, Rasul and 
Rogger’s (2018) project completion data set focuses on physical infrastructure projects, which are likely to 
be more comparable to each other and across organizations than Rasul, Rogger, and Williams’s (2021) data 
set of both physical and nonphysical outputs—but at the cost of less comprehensive coverage of government 
activity. The optimal place on this spectrum for any given measure of task completion naturally depends on 
FIGURE 17.2 A Spectrum of Task Completion Measures, with Selected Examples
 Broad, shallow, 
comparable
 Cross
government
 budget report, 
medium-term 
plan
 Organization-level
 nonphysical task 
completion coding 
(e.g., Rasul et al. 
2021)
 Infrastructure
 project completion 
coding
 UK Annual
 Report on
 Major Projects
 Narrow, deep,
 idiosyncratic
 Single-project
 performance
 audit
 (e.g., Rasul and 
Rogger 2018)
 Source: Original figure for this publication .
 379
 CHAPTER 17: GOVERNMENT ANALYTICS USING DATA ON TASK AND PROJECT COMPLETION
the analytical purpose for which it is being created. From the standpoint of advancing measurement, the aim 
is to find ways to surmount this trade-off by increasing both the comparability and the rigor of task comple
tion measures.
 CONCLUSION
 We conclude by returning to the question with which we opened: how do we know if governments are 
performing their functions well? In this chapter, we have sought to describe and demonstrate how to apply 
the task completion framework in order to answer precisely this question. The framework conceives govern
ment activity in such a way as to allow analysts to assess public performance in a standardized manner across 
organizations and types of activity. As such, it gives us a fuller and more accurate picture of government 
work, forces us to think more carefully about the characteristics of the tasks that different agencies perform, 
and facilitates comparison of performance on a large sample that spans many types of organizations.
 We have applied the framework to different categories of tasks in order to illustrate both its strengths and 
its limitations. In the case of tasks related to physical outputs, we have shown how data such as engineering 
assessments, annual progress reports, and budget reports can be merged with other data, such as manage
ment or user surveys, to provide a hitherto-inaccessible vision of the extent of project implementation and 
the quality of the work undertaken.
 Much of this work relies, at least partly, on data that already exist but have to be digitized or rendered 
usable in some other way. The existence of objective, external benchmarks—produced, for example, by 
experts such as infrastructure engineers—means that the development of projects of many different types 
can be mapped onto a comparable continuum. The strength of the evaluation of physical outputs is that 
analysts can produce a meaningful measure of completion that gives the user some sense of how task com
pletion maps onto public benefit. However, the weakness of the approach, as applied to physical outputs, 
is that the quality of task completion is often overlooked because it rests upon more complex, multifaceted 
assessments that are difficult to harmonize into a single indicator. Moreover, the reliability of such measures 
may be called into question where completion rates are reported by the same organizations that undertake 
the tasks themselves (although this can be counteracted to some degree if external audits of task reports are 
available to validate the measure).
 In the case of nonphysical outputs (such as auditing, planning, or awareness-raising activities), we have 
demonstrated how data may come from existing sources, such as progress reports, that need to be digitized 
or processed to be used for analysis. The strength of extending task completion assessments to nonphysical 
outputs is that this provides a much richer and fuller picture of the activities that governments engage in and 
allows for meaningful comparisons across departments. However, the task completion framework as applied 
to nonphysical outputs also suffers from the same potential misreporting concern associated with physical 
outputs and comes with additional challenges in terms of how to measure the quality of the tasks being 
completed. The challenges of measuring quality are distinct from those for physical outputs, in that quality 
is not necessarily overlooked but is more difficult to define. For example, how do you assess the quality of a 
health strategy objectively and in such a way that it is comparable with, for example, education strategies or 
f
 iscal strategies?
 T
 he task completion framework, in short, moves us in the right direction when it comes to measur
ing the performance of governments in a way that takes into account the full breadth of government 
activity. However, there is much room for improvement when it comes to the measurement of the qual
ity of the provision of both physical and nonphysical outputs. For physical outputs, expert benchmarks 
are often taken at face value without critical engagement with what the index or evaluation actually 
captures; whereas, for nonphysical outputs, benchmarks are often nonexistent, with no way to anchor 
quality assessments that makes them comparable across organizations. This is where we see the frontier 
in terms of the measurement of government performance; we need to expand the application of the task 
380
 THE GOVERNMENT ANALYTICS HANDBOOK
completion framework and complement this with greater attention to how technical benchmarks are used 
in the measurement of physical outputs and the development of workable benchmarks for the measure
ment of nonphysical outputs.
 NOTES
 T
 he authors gratefully acknowledge funding from the World Bank’s i2i initiative, Knowledge Change Program, and 
Governance Global Practice. We are grateful to Galileu Kim and Robert Lipinski for helpful comments.
 1. See, for instance, the World Bank’s World Governance Indicators, available at https://info.wordlbank.org/governance 
/wgi, and the Millennium Challenge Corporation scorecards—for example, on the website of the Millennium Challenge 
Coordinating Unit for Sierra Leone, http://www.mccu-sl.gov.sl/scorecards.html.
 2. Outputs are not to be confused with outcomes, or “the impacts on social, economic, or other indicators arising from the 
delivery of outputs (e.g., student learning, social equity).” OECD Glossary of Statistical Terms, s.vv. “output,” “outcome” 
(Paris: OECD Publishing, 2022), http://stats.oecd.org/glossary.
 3. Such indicators do not rely upon subjective citizen-survey responses, which are limited by their reliance on human judg
ment and prey to multiple biases and recall issues (Golden 1992), both from the researcher designing the study and the 
experts or citizen respondents evaluating the government.
 4. No evidence was found that completion levels differed significantly across auditors and agencies, with 94 percent of com
pletion rates being corroborated across coding groups (Rasul, Rogger, and Williams 2021, 265).
 5. The measure of unfinished projects is a “combination of projects still underway (on time or delays) and abandoned (tempo
rarily or indefinitely) in a given district” (Bancalari 2022, 10).
 6. It is important to note that their findings are relative to one another—that is, “organizations appear to be overbalancing 
their management practice portfolios toward top-down control measures at the expense of entrusting and empowering the 
professionalism of their staff” (Rasul, Rogger, and Williams 2021, 261).
 REFERENCES
 Ammons, David N. 2014. Municipal Benchmarks: Assessing Local Performance and Establishing Community Standards. 3rd ed. 
London: Routledge.
 Andrews, Rhys, George A. Boyne, Kenneth J. Meier, Laurence J. O’Toole Jr., and Richard M. Walker. 2005. “Representative 
Bureaucracy, Organizational Strategy, and Public Service Performance: An Empirical Analysis of English Local 
Government.” Journal of Public Administration Research and Theory 15 (4): 489–504. https://doi.org/10.1093/jopart/mui032.
 Bancalari, Antonella. 2022. “Can White Elephants Kill? Unintended Consequences of Infrastructure Development in Peru.” 
IFS Working Paper 202227, Institute for Fiscal Studies, London. https://ifs.org.uk/publications/can-white-elephants-kill -unintended-consequences-infrastructure-development.
 Bedoya, Guadalupe, Jishnu Das, and Amy Dolinger. Forthcoming. “Randomized Regulation: The Impact of Minimum Quality 
Standards on Health Markets.” Working paper, World Bank, Washington, DC.
 Bertelli, Anthony Michael, Eleanor Florence Woodhouse, Michele Castiglioni, and Paolo Belardinelli. 2021. Partnership 
Communities. Cambridge Elements: Public and Nonprofit Administration. Cambridge: Cambridge University Press.
 Bertelli, Anthony Michael, Valentina Mele, and Andrew B. Whitford. 2020. “When New Public Management Fails: Infrastructure 
Public-Private Partnerships and Political Constraints in Developing and Transitional Economies.” Governance: An International 
Journal of Policy, Administration, and Institutions 33 (3): 477–93. https://doi.org/10.1111/gove.12428.
 Boyne, George A. 2003. “What Is Public Service Improvement?” Public Administration 81 (2): 211–27. https://doi 
.org/10.1111/1467-9299.00343.
 Brown, Karin, and Philip B. Coulter. 1983. “Subjective and Objective Measures of Police Service Delivery.” Public Administration 
Review 43 (1): 50–58. https://doi.org/10.2307/975299.
 Carter, Neil, Rudolf Klein, and Patricia Day. 1992. How Organisations Measure Success: The Use of Performance Indicators in 
Government. London: Routledge.
 Denizer, Cevdet, Daniel Kaufmann, and Aart Kraay. 2013. “Good Countries or Good Projects? Macro and Micro Correlates 
of World Bank Project Performance.” Journal of Development Economics 105: 288–302. https://doi.org/10.1016/j 
.jdeveco.2013.06.003.
 381
 CHAPTER 17: GOVERNMENT ANALYTICS USING DATA ON TASK AND PROJECT COMPLETION
Dixit, Avinash. 2002. “Incentives and Organizations in the Public Sector: An Interpretative Review.” Journal of Human Resources 
37 (4): 696–727. https://doi.org/10.2307/3069614.
 Engel, Eduardo, Ronald Fischer, and Alexander Galetovic. 2013. “The Basic Public Finance of Public-Private Partnerships.” 
Journal of the European Economic Association 11 (1): 83–111. https://www.jstor.org/stable/23355049.
 Fenizia, Alessandra. 2022. “Managers and Productivity in the Public Sector.” Econometrica 90 (3): 1063–84. https://doi.org 
/10.3982/ECTA19 244.
 Finer, Herman. 1941. “Administrative Responsibility in Democratic Government.” Public Administration Review 1 (4): 335–50. 
https://doi.org/10.2307/972907.
 Flyvbjerg, Bent. 2009. “Survival of the Unfittest: Why the Worst Infrastructure Gets Built—And What We Can Do about It.” 
Oxford Review of Economic Policy 25 (3): 344–67. https://doi.org/10.1093/oxrep/grp024.
 Flyvbjerg, Bent, Mette Skamris Holm, and Soren Buhl. 2002. “Underestimating Costs in Public Works Projects: Error or 
Lie?” Journal of the American Planning Association 68 (3): 279–95. https://doi.org/10.1080/01944360208976273.
 Friedrich, Carl J. 1940. “Public Policy and the Nature of Administrative Responsibility.” In Public Policy: A Yearbook of the 
Graduate School of Public Administration, Harvard University 1: 1–20.
 Golden, Brian R. 1992. “The Past Is the Past—Or Is It? The Use of Retrospective Accounts as Indicators of Past Strategy.” 
Academy of Management Journal 35 (4): 848–60. https://doi.org/10.2307/256318.
 Goodhart, Charles A. E. 1984. “Problems of Monetary Management: The UK Experience.” In Monetary Theory and Practice: The 
UK Experience, 91–121. London: Red Globe Press. https://doi.org/10.1007/978-1-349-17295-5.
 Hefetz, Amir, and Mildred E. Warner. 2012. “Contracting or Public Delivery? The Importance of Service, Market, and Management 
Characteristics.” Journal of Public Administration Research and Theory 22 (2): 289–317. https://doi.org/10.1093/jopart/mur006.
 Ho, Alfred Tat-Kei, and Wonhyuk Cho. 2017. “Government Communication Effectiveness and Satisfaction with Police 
Performance: A Large-Scale Survey Study.” Public Administration Review 77 (2): 228–39. https://doi.org/10.1111/puar.12563.
 Honig, Dan. 2019. “When Reporting Undermines Performance: The Costs of Politically Constrained Organizational Autonomy 
in Foreign Aid Implementation.” International Organization 73 (1): 171–201. https://doi.org/10.1017/S002081831800036X.
 IPA (Infrastructure and Projects Authority). 2022. Annual Report on Major Projects 2021–22. Reporting to Cabinet Office and 
HM Treasury, United Kingdom Government. London: IPA. https://assets.publishing.service.gov.uk/government/uploads 
/system/uploads/attachmentdata/file/1092181/IPAAR2022.pdf.
 Kaddu, M., J. Aguilera, and L. Carson. n.d. Challenges to Policy Implementation in Uganda (Review of Policy Implementation in 
Uganda). London: International Growth Centre, London School of Economics and Political Science.
 Kelman, Steven, and John N. Friedman. 2009. “Performance Improvement and Performance Dysfunction: An Empirical 
Examination of Distortionary Impacts of the Emergency Room Wait-Time Target in the English National Health Service.” 
Journal of Public Administration Research and Theory 19 (4): 917–46. https://doi.org/10.1093/jopart/mun028.
 Khan, Muhammad Yasir. 2021. “Mission Motivation and Public Sector Performance: Experimental Evidence from Pakistan.” 
Working paper delivered at 25th Annual Conference of the Society for Institutional Organizational Economics, June 24–26, 
2021 (accessed February 8, 2023). https://y-khan.github.io/yasirkhan.org/muhammadyasirkhanjmp.pdf.
 Khwaja, Asim Ijaz. 2009. “Can Good Projects Succeed in Bad Communities?” Journal of Public Economics 93 (7–8): 899–916. 
https://doi.org/10.1016/j.jpubeco.2009.02.010.
 Lee, Soo-Young, and Andrew B. Whitford. 2009. “Government Effectiveness in Comparative Perspective.” Journal of 
Comparative Policy Analysis 11 (2): 249–81. https://doi.org/10.1080/13876980902888111.
 Lewis, David E. 2007. “Testing Pendleton’s Premise: Do Political Appointees Make Worse Bureaucrats?” The Journal of Politics 
69 (4): 1073–88. https://doi.org/10.1111/j.1468-2508.2007.00608.x.
 Lu, Jiahuan. 2016. “The Performance of Performance-Based Contracting in Human Services: A Quasi-Experiment.” Journal of 
Public Administration Research and Theory 26 (2): 277–93. https://doi.org/10.1093/jopart/muv002.
 Mani, Anandi, and Sharun Mukand. 2007. “Democracy, Visibility and Public Good Provision.” Journal of Development 
Economics 83 (2): 506–29. https://doi.org/10.1016/j.jdeveco.2005.06.008.
 Mansoor, Zahra, Garance Genicot, and Ghazala Mansuri. 2021. “Rules versus Discretion: Experimental Evidence on Incentives 
for Agriculture Extension Staff. ” Unpublished manuscript.
 Nistotskaya, Marina, and Luciana Cingolani. 2016. “Bureaucratic Structure, Regulatory Quality, and Entrepreneurship in a 
Comparative Perspective: Cross-Sectional and Panel Data Evidence.” Journal of Public Administration Research and Theory 
26 (3): 519–34. https://doi.org/10.1093/jopart/muv026.
 Olken, Benjamin A. 2007. “Monitoring Corruption: Evidence from a Field Experiment in Indonesia.” Journal of Political 
Economy 115 (2): 200–49. https://doi.org/10.1086/517935.
 Poister, Theodore H., and Gregory Streib. 1999. “Performance Measurement in Municipal Government: Assessing the State of 
the Practice.” Public Administration Review 59 (4): 325–35. https://doi.org/10.2307/3110115.
 Post, Alison E. 2014. Foreign and Domestic Investment in Argentina: The Politics of Privatized Infrastructure. Cambridge, UK: 
Cambridge University Press.
 382
 THE GOVERNMENT ANALYTICS HANDBOOK
Prendergast, Canice. 2002. “The Tenuous Trade-Off between Risk and Incentives.” Journal of Political Economy 110 (5): 
1071–102. https://doi.org/10.1086/341874.
 Propper, Carol, and Deborah Wilson. 2003. “The Use and Usefulness of Performance Measures in the Public Sector.” Oxford 
Review of Economic Policy 19 (2): 250–67. https://doi.org/10.1093/oxrep/19.2.250.
 Rainey, Hal G. 2009. Understanding and Managing Public Organizations. 4th ed. New York: John Wiley & Sons.
 Rainey, Hal G., and Paula Steinbauer. 1999. “Galloping Elephants: Developing Elements of a Theory of Effective Government 
Organizations.” Journal of Public Administration Research and Theory 9 (1): 1–32. https://doi.org/10.1093/oxfordjournals 
.jpart.a024401.
 Rasul, Imran, and Daniel Rogger. 2018. “Management of Bureaucrats and Public Service Delivery: Evidence from the Nigerian 
Civil Service.” The Economic Journal 128 (608): 413–46. https://doi.org/10.1111/ecoj.12418.
 Rasul, Imran, Daniel Rogger, and Martin J. Williams. 2021. “Management, Organizational Performance, and Task Clarity: 
Evidence from Ghana’s Civil Service.” Journal of Public Administration Research and Theory 31 (2): 259–77. https://doi.org 
/10.1093/jopart/muaa034.
 Rauch, James E., and Peter B. Evans. 2000. “Bureaucratic Structure and Bureaucratic Performance in Less Developed Countries.” 
Journal of Public Economics 75 (1): 49–71. https://doi.org/10.1016/S0047-2727(99)00044-4.
 Remington, Kaye, and Julien Pollack. 2007. Tools for Complex Projects. Aldershot, UK: Gower.
 Talbot, Colin. 2010. Theories of Performance: Organizational and Service Improvement in the Public Domain. Oxford: Oxford 
University Press.
 T
 homas, John Clayton, Theodore H. Poister, and Nevbahar Ertas. 2009. “Customer, Partner, Principal: Local Government 
Perspectives on State Agency Performance in Georgia.” Journal of Public Administration Research and Theory 20 (4): 779–99. 
https://doi.org/10.1093/jopart/mup024.
 Walker, Richard M., M. Jin Lee, Oliver James, and Samuel M. Y. Ho. 2018. “Analyzing the Complexity of Performance 
Information Use: Experiments with Stakeholders to Disaggregate Dimensions of Performance, Data Sources, and Data 
Types.” Public Administration Review 78 (6): 852–63. https://doi.org/10.1111/puar.12920.
 Williams, Martin J. 2017. “The Political Economy of Unfinished Development Projects: Corruption, Clientelism, or Collective 
Choice?” American Political Science Review 111 (4): 705–23. https://doi.org/10.1017/S0003055417000351.
 Woodhouse, Eleanor Florence. 2022. “The Distributive Politics of Privately Financed Infrastructure Agreements.” Unpublished 
manuscript.
 383
 CHAPTER 17: GOVERNMENT ANALYTICS USING DATA ON TASK AND PROJECT COMPLETION
=======================================================================================================================================================
 Utilising Big Data Analytics in the E-Government Health Sector  U. Nwogu, C. A. N. Nwachukwu, A. Cassia, U. E. Omenka, P. U. Sunday, R.U. Iheruo   Department of Computer Science, Federal Polytechnic Nekede, Owerri, Imo State, Nigeria uchenwogu@gmail.com  Abstract The  world  has  witnessed  massive  technological  improvements  in  the  Information  Technology  (IT)  sector,  giving  rise  to unprecedented  IT penetration  into every facet  of life  and daily  living. It  has affected  the way  we live  and interact,  the way businesses  are  conducted,  and  most  especially  how  the  government  makes  decisions  that  affect  its  citizenry.  Presently, government officials deploy IT techniques in their decision-making process to reach the ever-growing needs of their populace. The traditional approach to large data processing is proving difficult. Hence, the dawn of Big Data Analytics, a phenomenon that describes the processing of large volumes of datasets, being generated at a very high velocity, coming in various forms, and are largely unstructured.  This paper examines literatures  on Big Data Analytics  and its  application in e-government. A prototype framework which is divided into two will be proposed for its application. The first section will have the Hadoop infrastructure deployed for distributed storage in clusters, while the second section used a machine learning software – Waikato Environment for Knowledge Analysis (WEKA) for data mining. Finally, the different data mining algorithms provided by WEKA was explored and used in analyzing medical records obtained from the UCI online repository to demonstrate the data analysis of the proposed prototype. From the results of the analysis, J48 algorithm was used to build prediction trees, to ascertain patterns that determine the likelihood  of citizens  having breast cancer, and to generate predictions  rules that  will help  in curbing or  detecting breast cancer early among the populace.  Keywords: Big Data, Big Data Analytics, Data Mining, e-government, Knowledge Analysis    I. INTRODUCTION The government, as well as businesses, are collecting a lot of data these  days – movies, images,  and transactions. They do these  because data  is extremely  valuable.  We can  afford  to keep all our data on a disk because it is very cheap these days. Advancement  in  technology  has  led  to  the  development  of high-capacity memory storage disks at a very low price. Historically,  data  was  been  generated  and  accumulated  by workers  – employees  of companies  who  were entering  data into  computer  systems  manually  (data  entry  assistants). Technology evolved to the internet, where users can generate their data through websites like Facebook, Twitter, Wikipedia, Instagram, etc. These sets of data are larger than the first set by high magnitude. At first,  it was employees  entering data, then users entering their data. All of a sudden, the amount of data being generated scaled up significantly. Presently, there is a  third  order  in  progression,  because  now  machines  are collecting  data. The  buildings we see  all over  the cities, the highways, the rail lines, all have monitors as either cameras or sensors or both that collect data daily. The earth is also being surrounded by satellites that are monitoring the earth 24hours a  day taking  pictures  thereby accumulating  data. There  is a colossal amount of data being generated now. The  report  from  the  World  Economic  Forum  in  Davos, Switzerland  in  2012  stated  that  “the  volume  of  data  been generated in the world is increasing tremendously daily, by the constant interaction of billions of people using computers and mobile  devices.  Many  of  these  communications  take  place through the use of mobile devices by people in the developing world”.  It  further  stated  that  Scientists and  policymakers are beginning  to  comprehend  the  potential  for  feeding  these volumes  of data  into  actionable information  that can  aid in identifying  needs,  providing  services,  and  ‘predicting  and preventing’  crises for  the benefit  of low-income populations (World Economic Forum, 2012). We are in the information age which is driven by Information Technology and having devices such as sensors, cameras, and computers. With the global rise in internet usage, including the high  concentration  of  computational  resources  and  devices which allows sensing,  capturing, and  processing of real-time data  from  billions  of  connected  devices,  (serving  diverse applications) massive reservoir of datasets is being created as a result of regular collection of data. The volume of data we deal with has grown so large to petabytes and exabytes, and as data  keeps growing,  the various  forms in  which this  data is generated by different applications become richer than before. As a  result, traditional  relational databases  are challenged to capture, store, analyze,  and visualize data. This led  to a  new term  in describing  this very  large  data, and  it  is  called Big Data. The dawn of big data transformed the information technology world. Data is being generated now at a high rate than humans can  process,  and  issues  can  quickly  intensify  into  weighty events. The various  forms of data  being generated now  pose new privacy and security risks, and the extraordinary volume of information everywhere  makes it difficult to  locate where these issues,  risks, and  even useful information to  drive new value  and  revenue  are (Ballard,  et  al.,  2014). An  informed decision  is  always  the  best  decision,  which  is  why  large 206 
                                    Science View Journal Volume 3, Issue 1, 2022.                                                                                                                        ISSN (Online): 2734-2638  https://www.scienceviewjournal.org/ All rights reserved   organizations and governments are spending a huge amount of money  on  finding better  ways  to make  informed  decisions. These  developments  in  Information  Technology  led  to Electronic Government (e-Government). The  term  electronic  government  is  used  extensively  to characterize  the  use  of  information  and  communication technologies  in  public  sector  establishments.  Government agencies,  in  providing  the  best  of  services  to  the  general population, employ the use of analytics to drive their policies in  an  unstable  environment.  Analytics  and  data-driven decision-making  can  make  a  great  contribution  to  the achievement of goals set out by the government as they can to the  accomplishment  of  corporate  business  objectives (Davenport & Jarvenpaa, 2008). Gant  (2012) reported  that the  world faces  a great  challenge when it comes to how to integrate the strengths of computer-based information and communication technologies to help the government  of  developing  nations  to  serve  its  population efficiently and improve their human development conditions. E-government applications cut across a variety of services that produce large amounts of data in real-time. These could come in different forms such as video, audio, images, text, and from various sources  and government departments.  There is  a real need  for governments  to adopt  big data  analytics  in various domains  such  as  health  care,  fraud  prevention,  agriculture, transportation, education, etc.  Big  data  analytics  can  improve  the  efficiency  of  these government  services  by  the delivery  of  online services  and information  to  the  citizens,  and  also  protect  the  national borders  from attacks  by  criminals and  terrorists through  the prediction of  threats and  crimes. It is also effective  in fraud detection and tax collection. The  information  technology  awareness  of  people  and availability of network access has also increased significantly. With the booming economies, the number of people using e-government  applications  has  increased  enormously.  Current serving  e-government applications  are in  a silo  – traditional databases that are not optimized. They are designed to handle only structured data, and sharing data between applications is also  difficult  (Rajagopalan  &  Vellaipandiyan,  2013).  The collected data is used only for statistical purposes, and not for solving  critical  challenges  that  can  improve  the  quality  of government service. The challenges can be in form of: i. the volume of data, which deals with the size of the data being generated ii. the variety which defines the multiplicity of formats and sources of these data iii. the velocity which defines the rate and how fast these data are been generated This research paper aims at reviewing the challenges that big data pose in managing and analyzing medical/health records in an  e-Government  application  as  well  as  the  design  of  a prototype  application that  will utilize big data technology  to analyze  the  largely  unstructured  data.  This  study  will  also build  upon  the concept  of  data-driven  e-Government  which aims at integrating machine learning and artificial intelligence techniques  into  e-Government  systems so  as  to support  the governmental decision-making process. This will be achieved by: a) reviewing  and  identifying  current  problems  facing  the implementation and utilization of big data in e-government applications b) designing a new framework for utilizing big data analytics in  an  e-government  application  thereby  facilitating  the efficient mining of knowledge c) demonstrating  big  data  analytics  using  WEKA  machine learning software  II. LITERATURE REVIEW Big data is not an entirely novel idea but an emerging concept. However,  initial ideas  about big  data  were restricted  to  big ICT  organizations  such  as  Google,  Yahoo,  Microsoft,  and European  Organization  for  Nuclear  Research  (CERN).  Big Data  is defined  based  on its  features,  as no  clear  definition exists  (Zaslavsky,  Perera,  &  Georgakopoulos,  2013).  The name "Big Data" originated as a tag for a class of technology based on high-performance computing, as initiated by Google in  the early  2000s  (Hopkins &  Evelson, 2012).  It opens  an exciting opportunity for governments and companies to utilize these massive  amounts of  data being created for the good of society. So many researchers  have defined Big Data  – these definitions will be reviewed in the next section as the research themes  of  Big  Data.  Where  and  how  technology  can  be applied  in  an  e-government  application  (health  sector)  will also be discussed.  The  major  reason  for  the existence  of  the  phenomenon  ‘Big Data’  is  the  present  degree  to  which  information  is  been generated and presented. Wigan & Clarke (2013) opined that Big Data  does not only refer  to particular large  datasets, but also to data collections that amalgamate many datasets from various sources, and even to the methods used to manage and analyze the  data. Data have gone from stack to a flow, from something  stationary  and  static  to  something  fluid  and dynamic.  McKinsey  Global  Institute  noted  that  data  have become  a  torrent  flowing  into  all  sectors  of  the  universal economy,  and  defined  Big  Data  as  “datasets  that  are very large  making  it  difficult  for  traditional  database  tools  to manage and analyze” (Manyika, et al., 2011).  Big data has the potential to impact the world as the Internet evolves. More data can now be  collected on  a small storage device with a large storage capacity and can also be shared at the speed of light. The endpoint of Big Data is turning data into  information,  then  turning  the  information  into intelligence,  and  finally  taking  action  based  on  that intelligence.  Going  forward  into  a  data-centric  society,  Big Data will be a very important tool. In the past, we had small data  and  analyzed it  with  relational  database  systems using data  mining  and  warehousing.  Now  that  we  have  large amounts of data streaming  daily, we could do  things that we could not do  when we had  a small amount of data. The only 207 
                                    Science View Journal Volume 3, Issue 1, 2022.                                                                                                                        ISSN (Online): 2734-2638  https://www.scienceviewjournal.org/ All rights reserved   way the world is going to solve the various global challenges is with the effective use of data. We have more data now; we can see  something we could not see when we  had a smaller amount of  it. More data  does not just  let us see  more of the same thing we were already looking at, more data allows us to see new, better, and different things like patterns.  The  visible  gains  of  big  data,  artificial  intelligence,  and machine learning in society have begun to positively drive the transition  towards  a  data-driven  public  sector.  Decision-making in  the  public  sector  is  in  an embryonic  stage  of  a revolution  owing  to  the  inclusion  of  these  new technological  innovations.  Research  has  revealed  that  data-driven  e-government  policies  advance  socio-economic development  in  some  nations  (Agbozo  &  Asamoah,  2019). They also opined that data-driven e-government is capable of building resilient societies, and it is essential for the regard for citizen privacy, data integrity, transparency, and user need to be  at  the front  and  center of  initiatives  in order  to  build  a viable data-driven e-government ecosystem. Singh  &  Singh (2012)  suggested  that Big  Data  spans  three dimensions,  and they  are Volume  (The size  of data  is now larger than  terabytes and  petabytes), Velocity  (Data velocity measures  the  speed  of  data  creation,  streaming,  and collection), and Variety  (a measure of  how rich data  is been represented).  Yiu (2012)  summarized the  benefits associated with Big Data to society into the following keywords: sharing, learning,  and unification  of large  datasets,  target marketing, and advertising.   Big Data and E-Government The  dawn  of  Web  2.0  has  created  much  enthusiasm  for reinventing  governments.  The new  trend  of digitization  has seen  a  paradigm  shift  in  government  agencies,  as  they  are seeing massive growth in the volume of data being generated. Big  Data  improves  decision-making  and  increases organizational efficiency and effectiveness. McKinsey Global Institute  estimates  that  government  administration  in developed  economies of  Europe could  save  more  than  €100 billion  (£72  billion)  in operational  efficiency  improvements alone by using big data (Manyika, et al., 2011). In 2009, the United  States  government  launched  an  e-government  arm known  as  “data.gov”,  as  a  step  toward  government transparency  and  accountability  to  cover  the  transportation, economy, health  care, education, and  human services sectors of  the  government  (Kim,  Trimi,  &  Chung,  2014).  E-government refers to the use of new and emerging technology resources such as the internet, and mobile phones to improve the workings of government  (Rajagopalan &  Vellaipandiyan, 2013).  According  to  Musau,  et  al.  (2011)  e-government involves  using Information  and Communication  Technology (ICT)  tools  to  transform  both  backend  and  frontend government  processes.  They  also  noted  that  full implementation  of  this  technology  can  create  solid improvement in offering better services to their citizens. Big  data  is  the  ambiguous  technology  of  today’s IT development,  springing  up  opportunities  for  setting  up  e-governance in developing countries. In spite of its challenging deployment  in  developing  nations,  the  researchers recommended that government should emphasize  the priority of  the  migration  to  an  e-governance  system  while  creating awareness  programs  and  capacity  building  programs  are necessary for the general population to embrace the necessary change, and  IT policy and  regulations should  be IT friendly (Sahani & Thakur, 2021). Big  Data  analytics  can  process  these  structured  and unstructured  data  and  deliver  better  service  to  its  citizens (Rajagopalan  & Vellaipandiyan,  2013).  They also  identified the  benefits of  e-government as  increased transparency,  less corruption, greater  convenience, revenue growth,  low cost of running  services, and  a decrease  in time  and effort.  Agbozo (2018)  strongly  supported  the  migration  into data-driven  e-government  in  order  to  meet  the  UN  set  sustainable development  goals, as  it is  relevant  for decision  and policy making  to  support  the  governmental  processes  as  well  as impact the development and livelihood of citizens who are the main recipients of such public sector innovations.  Lachana, Alexopoulos, Loukis, & Charalabidis (2018) in their research  work  opined  that some  ICTs,  which  have  affected meaningfully the evolution of other important economic and social activities as well, have affected critically the evolution of  e-Government  and  have  motivated  the  rise  of  new generations of  tools used,  such as  social media  (which have been  quite  influential  for  the  evolution  of  the  electronic content  publishing,  as  well  as  the  customer  knowledge management, were the main driver of e-Government 2.0); the IoT  and  the  big  data  analytics  (which  have  been  quite influential  for  the  evolution of  the  industrial  manufacturing and  the  customer  knowledge  management,  seem  to  be important  for  the  emergence  of  Government  3.0).  Ogbuju, Taiwo, Ejiofor, & Onyesolu (2018) clearly demonstrated that data-driven  decision-making  is  the  best  way  for  the government  to  effectively  meet  the  needs  of  its  ever-rising population, and big  data holds the  answer to the question of data explosion  in Nigeria.  Their research  revealed that  apart from having data  processed and getting  information as  done with  conventional  database  environments,  the  government needs insights  for decision making;  and insights can  only be gotten  by  precise  analytics  of  both  raw  datasets  and information.  This  can  only  be  made  possible  through  the deployment of big data analytics and infrastructure across all government channels.  Big Data Technologies Big  Data  is  always  associated  with  the  technology  that facilitates  its  application.  Big  Data  technologies  can  be defined  as  “innovative  technologies  in  data  extraction,  and their designs designed such that values in data can be handled efficiently  by  regulating  the  different  characteristics  of datasets”  (Mathur, Sihag,  Sharma, &  Sharma, 2014).  These massive datasets from different sources and which also come in  different forms  pass through  a lot  of phases  to get  value from it. Yi, et al. (2014) called this process  “the life cycle of big data”,  which comprises  several  phases ranging  from data 208 
                                    Science View Journal Volume 3, Issue 1, 2022.                                                                                                                        ISSN (Online): 2734-2638  https://www.scienceviewjournal.org/ All rights reserved   generation, data collection,  data aggregation, processing, and application delivery.  The  foremost technology  linked with  Big Data  Analytics is Hadoop, which is an open-source research project by Google, and  introduced  by  Apache  Software  Foundation  (Katal, Wazid, & Goudar, 2013). This aids the distributed processing of a large volume of datasets by  using a group of distributed machines and specific computer programming models (Mauro, Greco, & Grimaldi, 2015). According  to Sagiroglu  & Sinanc (2013) Hadoop offers the following components: a) HDFS (Hadoop  Distributed File System):  this is a  highly fault-tolerant distributed file system developed for storing large datasets with streaming data access patterns running on clusters. b) MapReduce:  a  parallel  programming  technique  for distributed processing that allows for massive scalability. c)  III. PROTOTYPE DESIGN A  prototype  is  the  preliminary  version  of  a  software  that communicates the developer’s ideas and  concepts to the user, then makes a better final design of the software based on the feedback  generated  from  the  users  (Witten,  Frank,  &  Hall, 2011). It is a miniature version of the software and is used to validate  the  system  requirements  (Pfleeger  &  Atlee,  2010), also to support the user interface design, and increase system functionality.  The  aim of  building  a system  prototype  is  to validate  the  system's  functionality  and  performance.  The prototype platforms enable a designer to test and validate the processing  competencies  of  a  system.  It  also  aids  the integration  of  software  and  hardware  before  the  physical system development (Wu & Wang, 2012). The  Prototype  for  the  implementation  of  the  proposed  Big Data Analytics was designed using Axure RP. Axure RP was chosen  as  the  software  for  designing  the  process model/prototype.  This  is  because  Axure  RP  offers  you  the wireframing, prototyping, and documentation tools required to make  conversant design  choices, persuade any  skeptics, and document your design (axure.com, 2015).    Figure 1: Block Diagram of the Proposed Big Data Analytics Prototype IV. METHODS  Data  analysis  is  a  continuous  process  that  facilitates  the conversion of data into  information and knowledge, and  also to  unearthing  hidden  patterns. The  massive,  ever-increasing amount of data being generated  daily by different  firms and electronic  devices,  makes  getting  effective  real-time information  more  and  more  crucial.  Analysis  of  these  data includes  statistical  analysis,  simple  SQL  query,  and  data mining.  The dataset used in this work was gotten from the University Medical Centre, Institute of Oncology, Ljubljana, Yugoslavia, available  at the  UCI machine  learning  repository  (Lichman, 2013). This  data set domain  is breast cancer, and  it includes 201 instances  of one  class and  85 instances  of another  class.  There are  altogether 286 instances, described by  9 attributes, some of which are linear and some are nominal. Data mining will be used to extract useful and actionable information based on chosen criteria for investigation: a) The age range that is most likely to have breast cancer b) Which  degree  of  a  malignant tumor  is  more  likely  to reoccur after treatment c) Is radiation the best form of treatment? d) The  correlation  between  tumor  size  and  recurrence events Data in most cases are far from being in perfect condition for processing. Since most times they are integrated from different sources, data quality issues need to be addressed. The raw data needs  to  be  processed  and  cleaned  before  it  is  ready  for analysis. This is because quality data provides quality mining results for a quality decision-making process. WEKA  was chosen  for  the analytical  purpose  of this  work because  it  is  open-source  software  issued  under  the GNU General Public License and  it's a flexible application for  Big Data.  It  is  easy  to  understand  and  readily  available  for download with good user support forums. WEKA helps users 209 
                                    Science View Journal Volume 3, Issue 1, 2022.                                                                                                                        ISSN (Online): 2734-2638  https://www.scienceviewjournal.org/ All rights reserved   to  extract  valuable information  and  patterns from  data,  and enable  them  to  simply  ascertain  a  suitable  algorithm  for creating a precise prediction model from it.  Dataset into WEKA Explorer After the manual cleaning of the dataset  and converting it to the WEKA  file format (.arff), the  dataset will  be loaded  into the WEKA workbench for pre-processing and further analysis.    Figure 2: WEKA Explorer GUI   As the figure shows above, the breast cancer dataset has 286 instances  and  9  attributes  called  class,  age,  tumour  size, inv_nodes,  node_caps,  deg_malig,  breast,  breast_quadrant, irradiat.  Information  about  a  selected  attribute  is displayed also  on  the  right-hand  corner  of  the  interface,  and  also  a histogram  to  depict  the  details  of  that  attribute.  All  the attributes listed are nominal, that is having a predefined finite set of values. Some selected WEKA algorithms were chosen for this project to analyze the dataset and also to know the algorithm that will be the best for classification. Then a prediction model will be built for  the prediction  of new instances. A  classifier will be chosen from each of the Bayes classifiers, Rules classifier and Trees classifier,  build classification models for  the Radiation attribute  and  the  class  (recurrence-events/no-recurrence-events) attribute.  V. RESULTS AND DISCUSSION Based on the criteria for investigation already stated, we will compare the results obtained from the three chosen classifiers to determine which best suits our prototype in determining if radiation is the best form of treatment for breast cancer. Table 1 below, shows how the different algorithms performed on the dataset concerning the radiation attribute.      Table  1:  Comparing  the  Classification  Results  for Radiation Attribute  NaíveBayes ConjunctiveRule J48 Time  taken  to build  model (sec) 0 0 0 Correctly classified instances 216 214 210 Incorrectly classified instances 70 72 76 Kappa statistic 0.2314 0.0575 0.1252 Mean absolute error 0.3146 0.3611 0.3456 ROC Area 0.709 0.531 0.58 
=======================================================================================================================================================
Menu
HomeGovernmentData Analytics and AI in Government Project Delivery
Government
Project Delivery
Corporate report
Data Analytics and AI in Government Project Delivery
Published 20 March 2024

Contents
1.	Executive summary
2.	About this report
3.	What do we mean by project data analytics and AI?
4.	What’s the wider context?
5.	What’s the opportunity?
6.	What’s the risk?
7.	Creating the conditions for success
8.	What are the next steps?
Print this page
1. Executive summary
Innovation in data analytics and AI could be transformative for project delivery.

The volume of project data created through and about public investment is our great strength and presents enormous potential.

We will work together to harness this potential to deliver better project outcomes.
We will give project delivery professionals the skills to make best use of data.
We will remove barriers to sharing data.
We will experiment together to put data at the heart of our delivery.
We will think big, start small and scale fast, across our diverse projects.
Together, we aim to put the UK at the front of this emerging discipline.

The accelerating rate of innovation in data analytics and Artificial Intelligence (AI) has the potential to significantly impact project delivery and the project delivery profession. It brings transformative opportunities but also some fundamental challenges. We need to grasp these head on.

The UK is already a leading project delivery nation, but to be at the forefront of shaping how project data analytics and AI are used in the public interest we, the project delivery community, will need to work together. This is why the Infrastructure and Projects Authority (IPA), the Central Digital & Data Office (CDDO), the Association for Project Management (APM) and the Major Projects Association (MPA) have come together to set out a framework for the way we will develop the use of project data analytics and AI to help deliver government projects.

There are an array of uses for project data analytics and AI across the diverse range of projects across government. Combined with the uncertainty surrounding the outcome of rapid technological change, this means that a top down, one-size fits all approach to exploiting data is neither possible, nor desirable. Instead, government and its partners will collaborate to create the right conditions for innovation to thrive and ensure that success is shared at scale across the projects ecosystem. Together we will take action to:

Empower and enable the project delivery community to experiment to find solutions for the specific challenges they face in their projects and programmes.
Leverage the benefits of this experimentation to boost performance across the board.
The first tranche of actions are grouped around five themes:

Data skills and capability at scale. We will update the Government Project Delivery Capability Framework, and the associated government accreditation scheme, to be clear on the skills and roles needed for the future. Working with APM, we will set out interventions to grow and deploy these skills.
Better data and availability. We will work together to develop a common set of standards and taxonomy for project data, building a foundation of FAIR[footnote 1] data. Through this, we’ll also set expectations on rights of ownership and access to data.
Evidence-based decision making. We will strategically and systematically build the infrastructure across government project data and tools to enable greater insight, benchmarks and the ability to predict performance.
Experimenting together. We will work across government to define and oversee a series of pilots to innovate and experiment. The best bets will be amplified and scaled at pace.
Data partnerships. We will continue to work with professional bodies, academia and industry, focusing on our shared objectives and putting the UK at the forefront of this new project delivery discipline.
Delivery against these commitments will be a collective effort led by the IPA, as the centre of expertise for project delivery in government, working with the Projects Council[footnote 2] and Heads of Profession in departments, CDDO, the professional bodies, academia and industry.

2. About this report
This is a joint report from the IPA, the CDDO, the APM and the MPA. The No.10 Data Science team has also input.

The content draws on a range of sources, including the discussions held at the IPA’s project data analytics and AI summit in Belfast in June 2023. In attendance at this summit were the authoring organisations, together with the project delivery Heads of Profession from across central government departments.

The intended audience of this report is primarily government project leaders. However, given the common issues faced and the interconnected nature of the profession, it is likely to be of interest to all project leaders in the UK.

3. What do we mean by project data analytics and AI?
Working together on this endeavour means we need a common language.

For the purpose of this paper, we are defining project data as any data used for selecting a project and for defining, monitoring and tracking its performance. This could include project registers, schedules, plans, budgets and forecasts, meeting minutes, reports, assurance findings etc. This definition will be refined to create a more detailed definition of project data, as outlined further in section 7.2.

Aligning with the APM’s ‘Getting Started in Project Data’[footnote 3], we are defining project data analytics as using project data to:

Automate routine project tasks
Predict future project performance
Help make better project decisions
AI as a term can mean a lot of things. We are adopting the ‘National AI Strategy’ definition: ‘Machines that perform tasks normally requiring human intelligence, especially when the machines learn from data how to do those tasks.’

4. What’s the wider context?
Exploiting advances in data science and AI is already a priority for the UK government and our partners. This report does not stand alone and purposely draws upon and amplifies other interconnected work, including:


4.1 Government strategy
The National Data Strategy[footnote 4] and the National AI Strategy[footnote 5] set out how the UK intends to position itself at the forefront of the data and AI revolution to increase productivity, boost trade, create jobs and revolutionise the public sector. The National Data Strategy identifies four pillars on which realisation of these potential benefits will depend: ‘data foundations’[footnote 6], ‘data skills’, ‘data availability’ and ‘responsible use of data’. These pillars also underpin the actions described in section 7 of this report. The strategy identifies transforming government’s use of data to drive efficiency and improve public services as a priority course of action. This can only be achieved through a whole-government approach to which government project delivery will align.
Transforming for a digital future: government 2022-25 roadmap for digital & data[footnote 7] produced by CDDO sets out the strategy for the digital transformation of government operation and service delivery. It outlines concrete and measurable commitments to deliver digital skills at scale and better data to power decision making from which government project delivery will also benefit.
A Generative AI Framework for HMG[footnote 8] has also been produced by CDDO, which provides guidance on using generative AI safely and securely. It sets out ten core principles for generative AI use in government and public sector organisations, and provides practical considerations for anyone planning or developing a generative AI solution.
i.AI[footnote 9] - Incubator for AI has been established by the Deputy Prime Minister to drive forward the AI capabilities in central government. A central team of technical experts will be empowered to:
Improve public services through targeted applications of AI
Upgrade AI capability by building data sharing and AI infrastructure for use across government
Upskill civil servants to help them apply AI in their own areas of policy or operational delivery
The Government Project Delivery Function Strategy[footnote 10] sets out strategic objectives for where we want to be as a function by 2025. One of these objectives is data-driven performance: using data, analysis and experience to drive continuing improvement in government project delivery planning, performance and outcomes. This report provides more detail on how we will deliver on this objective (see section 5).
The Government Functional Standard for Project Delivery[footnote 11] sets the expectations for the direction and management of portfolios, programmes and projects. Mandated for government departments and arm’s length bodies, the standard provides the current expectations for how information and data should be managed as well as for the wider project delivery practices which we will want to enhance through better use of technology.
Transforming Infrastructure Performance (TIP) Roadmap to 2030[footnote 12] is an IPA led change programme to drive a step change in infrastructure performance, enhancing productivity, reducing costs, and improving the sustainability of all projects across government. Data and insight is one of the key TIP themes and progress has been made in this area through the development of a cross-government benchmarking data service (see section 7.3), standardised project metrics and a digital maturity assessment.
4.2 Our partners
Alongside government, project professionals in industry, academia and public services have come together to develop UK practice through the Project Data Analytics Task Force (facilitated by APM and MPA). Key outputs from the taskforce include:

Transforming Project Performance with Data[footnote 13] sets out a vision for securing a tenfold improvement in project delivery performance.
Getting Started in Project Data[footnote 14] is a guide to support organisations along their journey into project data analytics. It proposes that every project delivery organisation should have a data strategy.
The Project Data Analytics Task Force Manifesto[footnote 15] highlights six key principles for unlocking the potential of data driven project delivery.
Developing Project Data Analytics Skills[footnote 16] provides guidance for the profession on how to embed new skills and capabilities, including the role of digital and data professionals in project management. It unpacks what it means to be a ‘data-literate project professional’.
The APM sponsored Project Data Advisory Group is pursuing a range of activities and interventions including the incorporation of AI and Data Analytics into the APM formal body of knowledge, in a refresh due to be delivered in 2025.

This collective body of work - from government and our partners - signals a shared focus on the unrivalled potential offered by project data analytics and AI.

5. What’s the opportunity?
Data-driven performance is one of the core objectives of the Government Project Delivery Function Strategy.

The use of data analytics and AI is also an important enabler of three of the strategy’s other core objectives: better outcomes, efficient modern delivery and influential leadership.


Better outcomes. Data analytics and AI have the potential to be game-changing enablers of better outcomes for UK citizens. Systematic aggregation, sharing and learning of lessons across the portfolio could improve outcome-focused decision making and approvals. It will help us set more realistic goals and temper optimism bias. It also presents the opportunity for better option selection, and indeed better project selection, allowing us to focus on those interventions which give us the biggest return on our investment.

Efficient Modern Delivery. ‘The Government Efficiency Framework’[footnote 17] defines efficiency as ‘being able to spend less to achieve the same – or greater – outputs, or to achieve higher outputs while spending the same amount’. Greater automation, stronger controls, evidence-based decision making, reducing duplication and working in a more streamlined way will undoubtedly mean we’re able to do ‘more with less’.

Influential Leadership. The UK has a track record of leading development and sharing of project management practice at the global level, both through government methodologies (such as HM Treasury’s Green Book and IPA’s Project Routemap) and industry, brought together by the professional bodies. The UK has the opportunity to lead the conversation and boost the use of this emergent technology for the benefit of project outcomes and project professionals.

The final core objective, skilled and valued people, is essential to enable data-driven performance. To make the most of data and technology, we must have the data and business change skills to draw on. These data skills range from basic data literacy to programming, data visualisation, analysis and database management. Section 7.1 sets out how we’ll begin to build this capability.

Examples of project data analytics and AI opportunities
Predictive analytics
AI can be used to forecast project outcomes. Using machine learning[footnote 18] models (a subset of AI techniques) it is possible to make predictions about how a project could develop in the future. This relies on high quality, representative historical project performance data.

Traditional natural language processing approaches
Natural language processing can help identify and classify relevant text from longer extracts. This technique could be used to automatically extract and categorise recommendations - for example, from an assurance report. This benefits both summarisation of the report in question, but also provides data for further analysis (such as the predictive analysis described above).

Large language models[footnote 19]
Although there are potential issues around feeding official or commercially sensitive information into publicly available large language models, they can still be used for other purposes. For example, generating generic risk registers or other project documentation which can form a starting point for more detailed work by the project team. With the right funding, infrastructure and expertise, open-source models can be hosted securely in closed loop environments, enabling the model to be further trained or fine-tuned specifically using project data. This would allow text, image and code summarisation and content generation to be tailored to the government project delivery context.

6. What’s the risk?
Alongside these opportunities, AI also poses risks. Following the AI Safety Summit[footnote 20], for the first time there is a shared consensus on these risks, and on the need for collaborative mitigating action. We know not all AI risks arise from the deliberate action of bad actors. Some emerge as an unintended consequence or from a lack of appropriate controls to ensure responsible AI use[footnote 21].

Perhaps the most immediate risk of the application of AI in project delivery is that insight from data is not fully understood, based on poor quality data or incorrectly applied models. For example:

Available data may be insufficient in either quality, quantity or both to support the application of AI. Consequently, outputs may not be robust enough to support decisions, leading project delivery professionals to make erroneous decisions on the basis of this incomplete or poor data.
AI models can deliver systematically biased results as a result of incorrect input assumptions. Biases can be introduced at any stage in the AI lifecycle, from collecting data that are not diverse or representative to decisions made in the model-building process. Without careful testing and efforts to minimise bias, human incurred viewpoints can be perpetuated resulting in project delivery professionals making poor decisions.
AI applications are often seen as a ‘black box’. It is difficult for many people to understand how they work in general terms, let alone the calculations and assumptions the AI is making. The risk here is twofold: outputs are either taken at face value or, conversely, they are treated with suspicion or discarded out of hand.
Our approach must also remain alert to the longer-term risks posed by the following key characteristics[footnote 22] of AI:

The ‘adaptiveness’ of the technology: AI systems are ‘trained’ to operate by inferring patterns and connections in data which are not easily discernible to humans. Through such training, AI systems often develop the ability to perform new forms of inference not directly envisioned by their human programmers.
The ‘autonomy’ of the technology: Some AI systems can make decisions without the express intent or ongoing control of a human, so it is unclear who is accountable for their autonomous actions.
In the short term, given our anticipated use cases and current level of maturity, the ‘adaptiveness’ and ‘autonomy’ of AI pose fewer immediate risks for project delivery but will be closely monitored as the technology develops and new use cases emerge.

As well as technical risks, we must also be mindful of the business risks associated with investment in new technology, particularly given current levels of maturity and capability.

In section 7.4 we set out our approach to defining our risk appetite, and governing decisions on investment of effort in innovation with an understanding of the risk implications.

7. Creating the conditions for success
There are an array of uses for project data analytics and AI across the diverse range of projects in the government portfolio. Combined with the uncertainty surrounding the outcome of rapid technological change, this means that a top down, one-size fits all approach to exploiting data is neither possible, nor desirable. Instead, government and its partners will collaborate to create the right conditions for innovation to thrive and ensure that success is shared at scale across the projects ecosystem.

Given the opportunities and risks presented by the use of project data and AI, doing nothing is not an option. However, trying to anticipate how the technology will develop and iterate would be counterproductive. Our initial approach, as we understand more about the potential of how project data can help us deliver better outcomes, should be about ‘no regrets’ actions. These will be a combination of:

Foundational building blocks on which future actions can be built (see 7.1 and 7.2)
Experimentation and innovation at local levels to generate actionable insight and ways of working (see 7.3 and 7.4)
Working together, across government and with industry, to highlight innovation, and scale best bets for wider benefits (see 7.5)
Under this decentralised approach there is an important empowering and enabling role for the IPA, Projects Council and CDDO, working with the professional bodies.

Departments, delivery bodies and individual projects may need support to boost their capacity and capability, where it is low. We will remove barriers to sharing data where appropriate, by developing common standards, infrastructure and processes, so that the potential of new technology can be realised. As well as supporting innovation, the IPA will act in a convening role across the project delivery community to ensure we can all learn quickly and scale up to drive improved performance across the board. We will also collaborate broadly, leveraging government’s already strong relationships with industry and academia.

Our efforts will be focused around five priority themes:

Data skills and capability at scale
Better data and availability
Evidence-based decision making
Experimenting together
Data partnerships
7.1 Data skills and capability at scale
The opportunities outlined in section 5 cannot be realised without rapidly and strategically growing data capability. Significant investment in upskilling is already underway across the profession and government. We are also working to consider how a data-led approach might impact project delivery roles and delivery models, as well as the capability required to manage this change.

Increasing data skills
Harnessing data skills at scale requires collective action. To incentivise this shared behaviour across government, the annual ‘One Big Thing’[footnote 23] initiative for 2023 focused on ‘data for all’.

Outside of government, project data analytics is increasingly being incorporated into standards, including the APM Body of Knowledge and APM Competence Framework, reflected in its qualifications and chartered standard. Reskilling for a data and digital-enabled world is also one of the six pledges of the ‘Manifesto for Data-Driven Projects’[footnote 24](see section 7.5).

As well as data skills, adapting to and embedding project data analytics and AI into our ways of working will require a concerted and significant change management effort.

Clear roles and career paths
Any significant change to our skills profile will have implications for roles and career paths. The ‘Digital, Data and Technology (DDaT) Profession Capability Framework’[footnote 25] sets out cross-government definitions of key data-related roles, many of which will be deployed within projects.

However, there remains considerable uncertainty about the impact of data analytics and AI on existing project delivery roles. Some may be significantly impacted, with role redefinitions required to include data analytics responsibilities and competencies, while entirely new roles are also likely to emerge.  APM has published a framework for ‘Developing Project Data Analytics Skills’[footnote 26] which uses personas to demonstrate how different roles within a project might have different skill levels, interests and motivations for using data.

The IPA, supported by Projects Council, will reflect on the DDaT and APM frameworks, consider what this means for the capabilities required by government project delivery professionals and update the ‘Project Delivery Capability Framework’[footnote 27], the learning offer and the associated accreditation scheme.

Delivery models
There will also be implications for project delivery models. Projects will need to consider the extent to which they draw upon third-party data specialists versus growing their own talent, either held centrally by the portfolio or embedded within the project team. As a minimum, having enough data skills in-house will be critical to being a ‘smart’ procurer of services and tools from the market. Government will work with its partners and professional bodies to articulate the range of possible alternatives.

7.2 Better data and availability
The effectiveness of any analysis and insight is wholly dependent on the quality of the underlying project data. As set out in the ‘National Data Strategy’, we know the true value of data can only be fully realised when we have:

Data foundations - fit for purpose, recorded in standardised formats on modern, future-proof systems and held in a condition that means it is findable, accessible, interoperable and reusable (FAIR).
Data availability - appropriately accessible, mobile and re-usable, through improved coordination and appropriate protections for the flow of data between organisations.
Responsible data - used in a lawful, secure, fair, ethical, sustainable and accountable way, while also supporting innovation and research.
These requirements are inherent to the use of data in all settings. In addition, there are specific challenges which relate to government projects:

The diversity of projects, in terms of type, sector, scale and complexity.
Differing levels of maturity across departments and suppliers, and varying degrees of private and third sector collaboration.
The sensitivity of project data - security, political and commercial - which understandably creates barriers to sharing both internally, between government projects, and externally, between government and industry.
Challenges of knowing where data is stored and who has reasonable rights to access it.
We recognise that the vast volume of data generated across government projects is our great strength. With over 600 public bodies, as well as local government and suppliers collecting data on their projects, its collation and analysis presents enormous potential.

A number of government frameworks have already been developed to enable a practical and co-ordinated approach.[footnote 28] Several private sector organisations are also investing effort to address the challenge of data quality. 

Building on these frameworks and the work already underway, the Government Project Delivery Function will lead the establishment of standards for project data to build the foundations for FAIR data that could, in time, be pooled across the data ecosystem. We will also consider the commercial sensitivities and our levers to set expectations on rights of ownership and access to data, with a view to removing or mitigating some of the current barriers to sharing.

7.3 Evidence-based decision making
If we have better, more available data, we can then begin to use it to our advantage. At the moment, our decision-making too often lacks a robust evidence base or is influenced by subjective human factors such as bias.

The IPA is in the process of moving its government major project data collection to a purpose-built Cabinet Office platform - the Government Reporting Integration Platform (GRIP). This will allow us to better tailor data collection to our requirements, and show how projects align to government policy priorities. The expansion of the data environment will also enable more sophisticated analysis within and across projects and portfolios, including the application of machine learning tools.

Pooling and sharing project data can help provide norms and predictions of performance. The IPA Benchmarking Data Service is a new cloud-based data platform developed to share project data, enabling departments to learn from historical data to better inform decisions on current and future projects. It will provide flexibility to benchmark at various levels - from individual projects, to different types of project, to the portfolio as a whole. It will grow and adapt over time, striking a balance between government priorities, materiality and user demand. Its value will depend on a coordinated approach, both within government, with private sector partners and academia.

These two data repositories, alongside the significant data sets from assurance reviews, provide the potential for a vast project data set within government which can and should be leveraged to provide improved, more timely insight on project performance, driving productivity and quicker decision making.

The Data Science team in No.10 (10DS) has been developing and testing innovative methods and enabling digital infrastructure to ensure that data, evidence, and analytics sit at the heart of central approaches. The IPA will continue to work with 10DS to apply innovation and new technologies, and to upskill people to fuel reliable data-led delivery.

Going forward, as clear use cases emerge (see section 7.4), we will want to develop or procure other tools. Doing this in a joined-up way and leveraging our collective bargaining power will be critical in enabling government to procure tools and services more efficiently.

7.4 Experimenting together
The rapid pace of change and varied nature of potential use cases means that bottom-up experimentation will be the key to success. Government departments, delivery bodies and their private sector partners will need to run trials and iterate solutions for the specific challenges they face in their projects and programmes.

The IPA and Projects Council, as the centre of expertise for project delivery in government, is in a unique position to capture and share the range of experiments underway across government, to learn, test and scale quickly. The MPA and APM are similarly well-placed to fulfil this role for major projects and across the whole UK project management profession respectively.

The government profession network has a key role to play in sponsoring structured pathfinder pilots in which to invest our collective effort and deciding which experiments should be scaled up for further benefit.

The IPA and Projects Council will convene experiments and conversations to:

Define use cases across the range of our programmes
Steer experimentation with different data sets and data skills to create insight
Inform investment in tools and skills to pool, collate and analyse data sets, using our collective buying power
Amplify and scale the most promising opportunities
Initially, given the risks of not understanding the tools and flaws in the data we are using, the focus will be on ‘no regrets’ low risk innovation to drive efficiency and productivity. As our understanding and confidence improves, we will concentrate on use cases that contribute to wider government priorities.

Wider adoption of the most successful experiments will likely require new business processes and ways of working. We will leverage the reach of the Projects Council and Heads of Profession network and Communities of Practice across the Government Project Delivery Function to champion and lead these changes.

7.5 Data partnerships
Leaders in government project delivery have an important role alongside this in amplifying and scaling innovation across the government portfolio (see section 7.4). To truly unlock the full potential benefit of project data-analytics and AI for public good, we will collaborate more broadly.

Government already has strong relationships with professional bodies, industry and academia, focusing on our shared objectives and putting the UK at the forefront of this new project delivery discipline.

One such important collaboration is the Project Data Analytics Taskforce, which produced a ‘Manifesto for Data-Driven Projects’. Government is committed to the six pledges underpinning the manifesto’s goal of progressing the adoption of data analytics in project delivery. Continuing to work with industry, we will embed these principles into our ways of working for collective benefit.

The manifesto pledges:

We use data analytics to bust project management myths and beliefs.
All projects are data designed and enabled.
We pool our data to maximise insights.
We collaborate on opensource data analytics solutions tackling priority challenges.
We re-skill for a digital and data-enabled world.
Data analytics is codified in all aspects of project delivery best practice and culture.
Other examples of data partnerships include:

Collaborating with professional bodies on thought leadership. Working with professional bodies such as APM, MPA, PMI, RICS, ICE and others will provide the thought leadership to stimulate and celebrate a data-driven transformation. A recent example is the APM ‘Developing Project Data Analytics Skills’ framework.

Sponsoring research by academia. Working with academia, we will explore the benefits and expand on our collective knowledge of the potential uses and risks of project data.

Working with industry to champion UK excellence in data driven project delivery. Working with industry, we will create an environment that incentivises sharing of data, tools and skills between government and private sector partners.

8. What are the next steps?
This paper sets our direction of travel to harness the benefits of project data and AI for government projects and wider UK project focused organisations. Delivery against these ambitions will be a collective effort led by the IPA and the Projects Council, working closely with Heads of Profession across government, CDDO, 10DS and the professional bodies.

We will iterate and be agile in our approach as our understanding of the opportunities and risks grow. Progress across all areas will not be linear, but will be informed by access to information, resources and tools.

Data skills and capability at scale. We will update the Government Project Delivery Capability Framework, and the associated government accreditation scheme, to be clear on the skills and roles needed for the future. Working with APM, we will set out interventions to grow and deploy these skills.

Better data and availability. We will work together to develop a common set of standards and taxonomy for project data, building a foundation of FAIR data. Through this, we’ll also set expectations on rights of ownership and access to data.

Evidence-based decision making. We will strategically and systematically build the infrastructure across government project data and tools to enable greater insight, benchmarks and the ability to predict performance.

Experimenting together. We will work across government to define and oversee a series of pilots to innovate and experiment. The best bets will be amplified and scaled at pace.

Data partnerships. We will continue to work with professional bodies, academia and industry, focusing on our shared objectives and putting the UK at the forefront of this new project delivery discipline.

FAIR data are data which meet principles of findability, accessibility, interoperability, and reusability (FAIR). ↩

The Projects Council comprises Chief Project Delivery Officers of central government delivery departments and is chaired by the IPA CEO as Head of Function. It is responsible for overseeing and managing the project delivery function within government. ↩

APM, Getting Started in Project Data (PDF, 1.57MB) (2022) ↩

GOV.UK, National Data Strategy (2019) ↩

GOV.UK, National AI Strategy (2022) ↩

The National Data Strategy uses the term ‘data foundations’ to mean data that is fit for purpose, recorded in standardised formats on modern, future-proof systems and held in a condition that means it is findable, accessible, interoperable and reusable. ↩

GOV.UK, Transforming for a digital future: government 2022-25 roadmap for digital & data (2023) ↩

GOV.UK, Generative AI Framework for HMG (2023) ↩

GOV.UK, i.AI (2023) ↩

GOV.UK, Government Project Delivery Function Strategy 2025 (2023) ↩

GOV.UK, Government Functional Standard for Project Delivery (PDF, 707KB) (2021) ↩

GOV.UK, Transforming Infrastructure Performance: Roadmap to 2030 (2021) ↩

MPA, Transforming project performance with data (PDF, 1.37MB) (2021) ↩

APM, Getting Started in Project Data (PDF, 1.57MB) (2022) ↩

MPA, Project Data Analytics Task Force Manifesto ↩

APM, Developing Project Data Analytics Skills (PDF, 1.09MB) (2023)  ↩

GOV.UK, The Government Efficiency Framework (2023) ↩

Machine learning is intelligent processing that involves computer algorithms that ‘learn from doing’. Machine learning trains systems to progressively identify the characteristics of items and patterns of data to provide insights and aid decision-making. ↩

Large language models are advanced artificial intelligence systems designed to generate human-like text at a scale and complexity that allows them to mimic natural language, translation and respond in various languages and across many domains. ↩

GOV.UK, AI Safety Summit (2023) ↩

Centre for Security and Emerging Technology, AI Accidents: An Emerging Threat (2021) ↩

The two characteristics - adaptivity and autonomy - are explained more fully in GOV.UK, A pro-innovation approach to AI regulation (2023). ↩

GOV.UK, One Big Thing: data upskilling for all civil servants (2023) ↩

MPA, A manifesto for data-driven projects (2023) ↩

GOV.UK, Government Digital and Data Profession Capability Framework (2023) ↩

APM, Developing Project Data Analytics Skills (PDF, 1.09MB) (2023)  ↩

GOV.UK, Project Delivery Capability Framework: Infrastructure and Projects Authority (PDF, 9.77MB) (2022) ↩

GOV.UK, Data Sharing Governance Framework (2022); GOV.UK, Data Maturity Assessment for Government (PDF, 1.94MB) (2023); GOV.UK, The Government Data Quality Framework (2020); GOV.UK, A pro-innovation approach to AI regulation (2023); GOV.UK, The Rose Book: Guidance on knowledge asset management in government (2021) ↩

Back to top
Is this page useful?
Yes
No
Report a problem with this page
Services and information
Benefits Births, death, marriages and care Business and self-employed Childcare and parenting Citizenship and living in the UK Crime, justice and the law Disabled people Driving and transport Education and learning Employing people Environment and countryside Housing and local services Money and tax Passports, travel and living abroad Visas and immigration Working, jobs and pensions
Government activity
Departments News Guidance and regulation Research and statistics Policy papers and consultations Transparency How government works Get involved
Help Privacy Cookies Accessibility statement Contact Terms and conditions Rhestr o Wasanaethau Cymraeg Government Digital Service
 All content is available under the Open Government Licence v3.0, except where otherwise stated
© Crown copyright